![参考大神文档](https://www.cnblogs.com/a747895159/articles/18071162)

[TOC]

# 1.Elasticsearch与 Lucene 的区别？

- Lucene是一个java 开发的全文检索引擎工具包,提供完整的查询引擎与索引引擎。solr和ES都是基于该工具包的一些封装。
- ES是基于Lucene的二次开发，ES每个实例都是一个分离的Lucene实例。ES在Lucene基础上提供一个分布式的、基于JSON的WEB服务。
    - shard 分片：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能
    - replica 副本：任何服务器随时可能故障或宕机，此时 shard 可能会丢失，通过创建 replica 副本，可以在 shard 故障时提供备用服务，保证数据不丢失，另外 replica 还可以提升搜索操作的吞吐量。 
>
> shard 分片数量在建立索引时设置，设置后不能修改，默认5个；replica 副本数量默认1个，可随时修改数量；
>

# 2.ES 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？

- ES的搜索引擎依赖底层的Lucene的文件缓存filesystem cache,如果给Lucene足够的内存，可以将所有的idx segment file 索引数据文件容纳在内存之中，性能非常高。当数据量很大时，几十亿条时，懵逼的发现第一次搜索5-10s,后面查询就快了几百毫秒。就是Lucene首次缓存了文件索引。

- 假如ES数据量1T,ES 3台，每台32G内存，Lucene内存32G。1T的数据量是每台320G左右，而每台Lucene内存才32G,也就是1/10的数据才可以放内存。然后执行查询操作大部分都是走磁盘，性能肯定差。要让ES性能好，最好是将数据量都可以放到Lucene内存中。
  假如有一行数据30多个字段,只有是几个字段参与搜索，80%的字段不参与搜索的。结果是占据Lucene上的filesystem cache的空间，缓存的有效数据越少。
  可以采用es+hbase(Mysql)架构，将参与查询的索引放到ES中，从ES查询到id,然后id到HBASE(Mysql)查询全部数据。

**优化方案**：

- 扩容，多增加几台机器，或者调大Filesystem Cache内存，缓存越大越好
- 缩容，缩小index 索引：只存储必要的查询字段索引，非必要字段通过主键方式到其他存储查询。
- 冷热分离：大量的访问很少、频率很低的数据，单独写一个索引；将大量的访问很大、频率很高的数据，单独写一个索引。
- 索引模型优化：
  - 1.使用keyword代替 int/long/numeric(对于keyword类型的term query，ES使用的是倒排索引。但是numeric类型为了能有效的支持范围查询，它的存储结构并不是倒排索引); 
  - 2.预先建立 mapping，而不是让 ES 自动生成数据类型，加速检索.
- 查询优化：
  - 1.分页查询性能优化,尽量不要深度分页。采用score-api或者search-after方式。
  - 2.能用term就不用match_phrase
  - 3.使用过滤器优化查询。ES有一个特殊的缓存过滤器缓存（filter cache）,只储存了哪些文档能与过滤器相匹配的相关信息，而且可供后续所有与之相关的查询重复使用。
    - query：查询操作不仅仅会进行查询，还会计算分值，用于确定相关度；

![详细参考地址](https://www.cnblogs.com/a747895159/articles/18052148)

# 3.假如一台机器内存64G,ES内存多少合适？

- ES是基于 Lucene二次开发的，Lucene的设计是把底层OS的数据缓存到内存中。Lucene的性能取决于和OS交互,如果内存太小，全文索引性能太差。
- 标准情况下50%的内容给ES，50%的内存给Lucene,用于文件缓存。
- ES内存不要超过32G,主要原因是JVM在内存小于32G的时候会采用**内存对象指针压缩技术**：
  - 在java中，所有的对象都分配在堆上,然后用指针引用它。这些指针的大小通常是CPU的字长大小,取决于处理器,一般32位或64位。但是64位的指针意味着更大的浪费,浪费内存无所谓，主要是更大的指针在主存与告诉缓冲器之间数据移动的时候,会占用更多的资源与带宽。
  - Java使用内存指针压缩技术来解决以上问题，它的指针不再是内存的精确位置，而是表示偏移量。也就意味着32位指针可以引用40亿个对象。
  - 当内存越过申请的边界 30-32G时，对象指针边长，会使用更多的CPU内存带宽，当内存40-50G的时候才相当于使用压缩技术时候的32G内存。
  - 如果你有台大机器内存,你可以多部署几个节点，仍然使用50%原则,32G内存给ES 32G内存给Lucene.


# 4.ES 使用存在的坑点

- ES 的每个分片（shard）都是lucene的一个index，而lucene的一个index只能存储21亿左右文档(即 Integer.MAX_VALUE - 12)，单个分片的大小在10G-50G之间。
- Text类型在存入 Elasticsearch 的时候，会先分词，然后根据分词后的内容建立倒排索引。部分场景根据keywork查询即可，去掉text字段，节约空间。
- 增加**routing key**,可以将请求分发到具体的shard，减少大量不必要的请求。
- 过多依赖ES聚合结果，ES聚合结果不是很精确的。
- 分桶多字段聚合查询容易引起OOM。ES对于这种嵌套聚合默认使用了深度优先规则。
- ES中更新操作存在近1s的延迟，主要是写数据是定时写入,refresh_interval默认为1s。

![](https://img2024.cnblogs.com/blog/1694759/202403/1694759-20240314104640990-487221941.png)

# 5.ES 节点类型介绍

**主节点（Master Nodes）**：node.master设置为True（默认）。通常建议设置3个专用的可竞争的主节点。这些节点负责集群的元数据管理，如索引的创建与删除、分片的分配、主副shard管理等。主节点不推荐参与数据的存储和检索，因此它们的硬件资源可以相对较轻。

**数据节点（Data Nodes）**：node.data设置为True（默认）。对于大多数数据密集型应用，建议将剩余的节点大部分配置为数据节点。数据节点负责存储索引数据，并执行搜索、聚合等操作。根据数据量和查询负载，可以灵活调整数据节点的数量。

**协调节点（Coordinating Nodes）**：node.master和node.data都设置为false。如果集群接收大量的客户端请求，可以考虑设置一些协调节点。这些节点不存储数据，主要负责接收客户端请求，并将请求路由到相应的数据节点。协调节点可以帮助平衡负载，提高集群的响应速度。


# 6.聊聊你们公司ES的集群架构，索引数据大小，分片有多少，以及一些调优手段 。


出库单明细   每个月 7亿，大概 150GB空间, 用了10个分片
库存日志    每个月 10亿 ,大概 120GB空间, 用了10个分片

总共有3个集群，每个集群11个节点(3主节点+8数据节点)。200多个别名索引，每日递增 1亿+数据。
大数据量场景，采用别名按月建立索引。
一般大数量每个索引5-10个分片(单分片的合适存储大小通常在10G到50G之间,当分片数量过多时，查询操作需要跨多个分片进行，这增加了查询的复杂性和延迟)，1个副本,同时索引设置分页方式最大可查数10000条。

### 索引设计调优：

   - 1）根据业务增量需求，基于模板+时间+rollover api滚动创建索引; 避免单个索引存储过大。
   - 2）使用别名进行索引管理；
   - 3）采取冷热分离机制，热数据存储到SSD，提高检索效率；冷数据(不会存在写入)定期进行shrink(缩小)操作,减少分片数,缩减存储；
   - 4）Mapping阶段充分结合各个字段的属性，是否需要检索、是否需要存储等; 仅针对需要分词的字段，合理的设置分词器

### 批量数据写入调优：

   -  1.写入前副本数设置为0;
   -  2.写入过程中：采取bulk批量写入;
   -  3.**尽量使用自动生成的id**。特定的_id将数据写入Elasticsearch时，Elasticsearch会检查对应的索引下是否存在相同的_id。这个操作会随着文档数量的增加而消耗越来越大，从而影响写入速率。
   -  4.写入后恢复副本数和刷新间隔;

### 查询优化：

   - 限制深度分页查询，使用 search_after 参数可以实现基于上一次查询结果的**游标分页**。这样，Elasticsearch只需要从上次的结束点开始扫描，大大提高了性能。
   - 使用过滤器优化查询；

# 7.倒排索引

正排索引：是以文档对象的唯一 ID 作为索引，以文档内容作为记录。适用于关系型数据库中的结构化数据查询，如基于字段的精确匹配、范围查询等
倒排索引：Inverted index，指的是将文档内容中的单词作为索引，将包含该词的文档 ID 作为记录。倒排索引的构建过程包括分词、建立单词词典、建立倒排列表。可倒排索引以大大提高了搜索的效率和准确性。适用于需要高效全文搜索和模糊查询的场景，如搜索引擎、日志分析、内容管理系统等
>
> * 倒排索引中的所有词项对应一个或多个文档.
> * 倒排索引中的词项 根据字典顺序升序排列.
>

![](https://img2024.cnblogs.com/blog/1694759/202403/1694759-20240314154046779-944507761.png)


# 8.Elasticsearch 集群脑裂问题？有哪些解决方法

通常是由于**网络问题**、**节点负载过高**、**资源限制或配置不当**等原因导致的产生 2个主节点。

- 优化网络配置 ：确保集群内各节点之间的网络通信稳定可靠，减少网络延迟和故障的可能性。
- 合理配置资源：确保每个节点都有足够的资源（如CPU、内存和磁盘空间）来处理请求和数据存储；
- 调整选举策略：通过调整Elasticsearch的选举策略来减少误判和脑裂的可能性。例如，增加discovery.zen.minimum_master_nodes参数的值，以确保在选举主节点时有足够的节点参与。
- 监控和告警：使用Elasticsearch自带的监控工具或其他第三方监控工具，实时监控集群的状态和性能指标。及时告警并处理。
- 当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题

# 9.Elasticsearch是如何实现master选举的？

ES算是AP模型，主节点仅负责集群元数据管理，如索引的创建和删除、分片的分配等。当主节点挂了，也不影响数据查询，部分新索引写入操作会受影响。

前提条件：
1、只有候选主节点（master：true）的节点才能成为主节点。
2、最小主节点数（min_master_nodes）的目的是防止脑裂。

- 选主是 ZenDiscovery 模块负责（Bully算法）的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分
- 对所有可以成为 master 的节点（node.master: true）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。
- 如果对某个节点的投票数达到一定的值**（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己**，那这个节点就是 master。否则重新选举一直到满足上述条件。

# 10.ES的写入流程：

**1、ES写数据的整体流程：**

![](https://img2024.cnblogs.com/blog/1694759/202403/1694759-20240314164704191-1968441957.png)

>* （1）客户端选择 ES 的某个 node 发送请求过去，这个 node 就是协调节点 coordinating node
>* （2）coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）
>* （3）实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node
>* （4）coordinating node 等到 primary node 和所有 replica node 都执行成功之后，最后返回响应结果给客户端。

**2、ES主分片写数据的详细流程：**

![](https://img2024.cnblogs.com/blog/1694759/202403/1694759-20240314164729881-1218204701.png)

![](https://img2024.cnblogs.com/blog/1694759/202409/1694759-20240914161519392-453727764.png)


（1）主分片先将数据写入ES的 memory buffer，然后定时（默认1s）将 memory buffer 中的数据写入一个新的 segment 文件中，并进入操作系统缓存 Filesystem cache（同时清空 memory buffer），这个过程就叫做 refresh；每个 segment 文件实际上是一些倒排索引的集合， 只有经历了 refresh 操作之后，这些数据才能变成可检索的。

>ES 的近实时性：数据存在 memory buffer 时是搜索不到的，只有数据被 refresh 到 Filesystem cache 之后才能被搜索到，而 refresh 是每秒一次， 所以称 es 是近实时的；可以手动调用 es 的 api 触发一次 refresh 操作，让数据马上可以被搜索到；

（2）由于 memory Buffer 和 Filesystem Cache 都是基于内存，假设服务器宕机，那么数据就会丢失，所以 ES 通过 translog 日志文件来保证数据的可靠性，在数据写入 memory buffer 的同时，将数据也写入 translog 日志文件中，当机器宕机重启时，es 会自动读取 translog 日志文件中的数据，恢复到 memory buffer 和 Filesystem cache 中去。

> 1.顺序写，默认采用同步刷盘策略，会降低写入性能。
> 2.为了平衡写入性能和数据持久化的需求，也支持异步刷盘策略。通过设置 `index.translog.durability=async`，会定期（默认每5秒）执行刷盘命令，将Translog中的数据写入磁盘。
> 3.translog文件默认 每30分钟 或者 阈值超过 512M 时，会触发刷盘 flush操作。

（3）flush 刷盘操作：将 memory buffer 中所有的数据写入新的 segment 文件中， 并将内存中所有的 segment 文件全部落盘，最后清空 translog 事务日志。

> ① 将 memory buffer 中的数据 refresh 到 Filesystem Cache 中去，清空 buffer；
> ② 创建一个新的 commit point（提交点），同时强行将 Filesystem Cache 中目前所有的数据都 fsync 到磁盘文件中；
> ③ 删除旧的 translog 日志文件并创建一个新的 translog 日志文件，此时 commit 操作完成


# 11.详细描述一下Elasticsearch搜索的过程。


- 1、搜索被执行成一个两阶段过程，我们称之为Query Then Fetch；

- 2、在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为from + size的优先队列。

> PS：在搜索的时候是会查询Filesystem Cache的，但是有部分数据还在MemoryBuffer，所以搜索是近实时的。
> 查询数据主副分片选择多种因素：数据的同步状态、查询需求、集群负载、以及数据的可用性和一致性要求、轮询策略等

- 3、每个分片返回各自优先队列中 所有文档的ID和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。

- 4、接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个GET请求。每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。

> 补充：Query Then Fetch的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch增加了一个预查询的处理，询问Term 和 Document frequency，这个评分更准确，但是性能会变差。


# 12.详细描述一下Elasticsearch更新和删除文档的过程。

1、删除和更新也都是写操作，但是Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；

2、磁盘上的每个段都有一个相应的del文件。当删除请求发送后，文档并没有真的被删除，而是在del文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在del文件中被标记为删除的文档将不会被写入新段。

3、如果delete记录过多，可能会占用过多的磁盘空间，并增加清理工作的负担。这可能会对Elasticsearch集群的性能产生一定的影响。建议在使用Elasticsearch时，尽量优化更新操作，减少不必要的更新，以降低delete记录的数量。
  - 删除的数据默认情况下不会立即从磁盘上清除，而是会保留一段时间，以便于复制和恢复的需要.
  - Elasticsearch会在某个时间点上执行一个过程，称为"refresh"，来清理这些已删除但尚未提交的文档。这个过程是隐式的，不需要用户干预.

4、在新的文档被创建时，Elasticsearch会为该文档指定一个版本号，当执行更新时，旧版本的文档在del文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。


# 13.scroll api  与 search_after 游标深度分页之间的区别

推荐使用search_after进行分页，原因如下：

- 实时性：search_after每次请求都会返回最新的结果，适用于需要实时搜索的场景。相比之下，Scroll API在滚动开始时就已经确定了结果集，不会反映后续的索引变更。

- 资源管理：Scroll API需要维护一个滚动上下文，这可能会占用服务器资源。如果不及时清理滚动上下文，可能会导致资源泄露。而search_after不需要维护这样的上下文，对资源的管理更为简洁。

- 深度分页：对于深度分页场景，search_after和Scroll API都是较好的选择。但考虑到Scroll API的实时性和资源管理问题，search_after在这种情况下可能更具优势。

- 避免重复和遗漏：虽然使用search_after时需要保存上一次搜索的最后一个文档的排序键，但只要确保排序字段唯一或能够唯一标识每个文档，就可以避免重复或遗漏文档的情况。


# 15.并发情况下，Elasticsearch 如果保证读写一致？

（1）对于更新操作：可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖
>
> 每个文档都有一个_version 版本号，这个版本号在文档被改变时加一。Elasticsearch使用这个 _version 保证所有修改都被正确排序，当一个旧版本出现在新版本之后，它会被简单的忽略。
> 利用_version的这一优点确保数据不会因为修改冲突而丢失，比如指定文档的version来做更改，如果那个版本号不是现在的，我们的请求就失败了。
>

（2）对于写操作，`action.write_consistency`性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，副本将会在一个不同的节点上重建。
>
> * one：写操作只要有一个primary shard是active活跃可用的，就可以执行
> * all：写操作必须所有的primary shard和replica shard都是活跃可用的，才可以执行
> * quorum：默认值，要求ES中大部分的shard是活跃可用的，才可以执行写操作
>

（3）对于读操作，可以通过设置搜索请求参数 _preference 为 primary 来查询主分片，确保文档是最新版本。

# 16.聊聊：字典树的大致思想和性质？

字典树又称单词查找树，**Trie 树** ，是一种树形结构，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。

它的优点是：**利用字符串的公共前缀** 来减少查询时间，最大限度地减少无谓的字符串比较，**查询效率比哈希树高** （空间换时间）。

|数据结构|优缺点|
|---|---|
|排序列表Array/List|使用二分法查找，不平衡|
|HashMap/TreeMap|性能高，内存消耗大，几乎是原始数据的三倍|
|Skip List|跳跃表，可快速查找词语，在lucene、redis、Hbase等均有实现。相对于TreeMap等结构，特别适合高并发场景|
|Trie|适合英文词典，如果系统中存在大量字符串且这些字符串基本没有公共前缀，则相应的trie树将非常消耗内存|
|Double Array Trie|适合做中文词典，内存占用小，很多分词工具均采用此种算法|
|Ternary Search Tree|三叉树，每一个node有3个节点，兼具省空间和查询快的优点|
|Finite State Transducers (FST)|一种有限状态转移机，Lucene 4有开源实现，并大量使用|

Trie的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。
