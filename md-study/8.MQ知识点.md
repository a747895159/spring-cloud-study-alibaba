//延迟消息原理
https://www.cnblogs.com/heihaozi/p/13259125.html

//博客园文档
https://www.cnblogs.com/a747895159/articles/15544495.html

// 阿里云RocketMQ 使用指南
https://help.aliyun.com/document_detail/293595.html

// 源码分析RocketMQ消息消费机制----消费者拉取消息机制
https://blog.csdn.net/prestigeding/article/details/78885420


RocketMQ 顺序消息实现机制

https://www.jianshu.com/p/0ff1b6a3da36

-----------------

[TOC]


# 1. RocketMQ 业务名词含义

- NameServer路由注册中心
    - 是一个非常简单的Topic路由注册中心，是一个几乎无状态节点，可集群部署，节点之间数据不同步。其角色类似Dubbo中的zookeeper，是AP模式，支持Broker的动态注册与发现。
    - 路由信息管理，每个NameServer保存Broker集群的**所有topic信息、以及topic对应的Broker里的队列信息**。
    - 提供心跳检测机制，检查Broker是否还存活。
    - Producer和Conumser通过NameServer(随机一个)建立长连接，就可以知道整个Broker集群的路由信息，从而进行消息的投递和消费。即使某个NameServer下线了，Producer,Consumer仍然连接其他NameServer动态感知Broker的路由的信息。


- Broker

    - 分为Master与Slave，主从通过指定相同的BrokerName，1对多关系，不同的BrokerId 来定义，BrokerId为0表示Master，非0表示Slave。**只有BrokerId=1的从服务器才会参与消息的读负载**。Master也可以部署多个。
    - 每个Broker与NameServer集群中的**所有节点建立长连接，定期每30秒注册Topic信息到所有NameServer**。
    - **Broker维护消费者与队列的订阅关系**，并将订阅关系扩散到整个Broker集群。所有消息默认3天后会被自动删除。

- Producer

    - 与NameServer集群中的其中一个节点（随机选择）建立长连接，定期每30秒从NameServer获取Topic与Broker路由信息。
    - 再跟 Topic 涉及的**所有主Broker(master)** 建立长连接，且定时(每30s)发送心跳。Producer完全无状态，可集群部署。

- Consumer

    - 与NameServer集群中的其中一个节点（随机选择）建立长连接，定期每30秒从NameServer获取Topic路由信息。

    - **并向提供Topic服务的Master、Slave建立长连接，且定时向Master、Slave发送心跳**。

    - Master与Slave都可以订阅消息，消费者在向Master拉取消息时，Master服务器会根据拉取偏移量与最大偏移量的距离（判断是否读老消息，产生读I/O），以及从服务器是否可读等因素建议下一次是从Master还是Slave拉取。



![](https://img2024.cnblogs.com/blog/1694759/202407/1694759-20240729192946720-1501032242.png)

# 2. Topic路由注册与剔除流程

+ Broker 每30s向 NameServer 发送心跳包，心跳包中包含主题的路由信息(主题的读写队列数、操作权限等)，NameServer 会通过 HashMap 更新 Topic 的路由信息，并记录最后一次收到 Broker 的时间戳。
+ NameServer 以每10s的频率清除已宕机的 Broker，NameServer 认为 Broker 宕机的依据是如果当前系统时间戳减去最后一次收到 Broker 心跳包的时间戳大于120s。
+ 消息生产者以每30s的频率去拉取主题的路由信息，即消息生产者并不会立即感知 Broker 服务器的新增与删除。
+ 消费组下部分消费者要维护与与所有关联的Broker的心跳(30秒)，如果Broker宕机，则会自动清理下线Broker。



# 4. RocketMQ消费端启动流程

![](https://img2024.cnblogs.com/blog/1694759/202404/1694759-20240429101912780-356187072.png)

```
public void start() throws MQClientException {

    synchronized (this) {
        switch (this.serviceState) {
            case CREATE_JUST:
                this.serviceState = ServiceState.START_FAILED;
                // If not specified,looking address from name server
                if (null == this.clientConfig.getNamesrvAddr()) {
                    this.mQClientAPIImpl.fetchNameServerAddr();
                }
                //启动客户端Netty，可以访问外部
                this.mQClientAPIImpl.start();
                //一些列的定时任务： 1.获取nameServer地址;2.更新topic队列信息定时任务;3.清理下线Broker信息，向所有Broker发送心跳;4.持久化消费者位移定时任务;5.启动调整消费者消费消息线程个数(暂未实现)的定时任务；
                this.startScheduledTask();
                // 启动拉去消息服务 pullMessageService
                this.pullMessageService.start();
                // 启动再平衡服务rebalanceService
                this.rebalanceService.start();
                // 设置生产者product相关信息
                this.defaultMQProducer.getDefaultMQProducerImpl().start(false);
                log.info("the client factory [{}] start OK", this.clientId);
                this.serviceState = ServiceState.RUNNING;
                break;
            case START_FAILED:
                throw new MQClientException("The Factory object[" + this.getClientId() + "] has been created before, and failed.", null);
            default:
                break;
        }
    }
}
    
```





# 5. RocketMQ如何负载均衡

## 生产者

生产者按照**索引递增随机取模**方式，有一个 sendLatencyFaultEnable 开关变量，如果开启，在随机递增取模的基础上，再过滤掉 not available 的 Broker 代理，是指对之前失败的，按一定的时间做**退避**。**latencyFaultTolerance 机制是实现消息发送高可用的核心关键所在。**
![](https://img2024.cnblogs.com/blog/1694759/202407/1694759-20240729222344740-18310231.png)

## 消费者（RebalanceService）

**负载算法**：平均分配策略(默认)、环形分配策略、一致性哈希分配策略、机房分配策略(优先consume与Queue同机房分配)

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211116145625523-1563475991.png)

+ RebalanceService 负责消息消费队列的负载，**默认以20s**的间隔按照队列负载算法进行队列分配，如果此次分配到的队列与上一次分配的队列不相同，则需要触发消息队列的更新操作：
    * ①**如果是新分配的队列，则创建 PullReqeust 对象(拉取消息任务)，添加到 PullMessageService 线程内部的阻塞队列 pullRequestQueue 中**。如果该队列中存在拉取任务，则 PullMessageService 会向 Broker 拉取消息。
    * ②**如果是上次分配但本次未分配的队列，将其处理队列 ProcessQueue 的状态设置为丢弃**，然后 PullMessageService 线程在根据 PullRequest 拉取消息时首先会判断 ProcessQueue 队列的状态，如果是已丢弃状态，则直接丢弃 PullRequest 对象，停止拉取该队列中的消息，否则向Broker 拉取消息，拉取到一批消息后，提交到一个处理线程池，然后继续将 PullRequest 对象添加到 pullRequestQueue，即很快就会再次触发对该消息消费队列的再次拉取，这也是 RocketMQ 实现 PUSH 模式的本质。

- 在 PUSH 模式下，PullMessageService拉取完一批消息后，将消息提交到线程池后会“马不蹄停”再去拉下一批消息，如果此时消息消费线程池处理速度很慢，处理队列中的消息会越积越多，占用的内存也随之飙升，最终引发内存溢出，更加不能接受的消息消费进度并不会向前推进，因为只要该处理队列中偏移量最小的消息未处理完成，整个消息消费进度则无法向前推进，如果消费端重启，又得重复拉取消息并造成大量消息重复消费。RocketMQ 解决该问题的策略是引入消费端的限流机制。



# 6. RocketMQ推拉实现机制

- Push模式：MQ Server主动将消息推送给消费者。
    - **优点是实时性高，缺点是当消息量大时，对消费者的性能要求较高，容易造成消息堆积。‌**

- Pull模式：消费者主动从MQ Server中拉取消息。
    - **优点是消费者决定何时拉取消息，有较好的控制权，压力相对较小；缺点是消费者定时或按需去拉取消息，实时性较低。**

-  pullMessage函数的参数是 final PullRequest pullRequest ，伪Push方式，**通过“长轮询”方式达到 Push效果**的方法。长轮询就是在Broker在没有新消息的时候才阻塞，**阻塞时间默认设置是 15秒，有消息会立刻返回**。
-  每次Pull一批消息放在队列中，然后交给子线程队列一个个处理。
    - 长轮询拉取有流量控制 防止消息在消费者堆积。每个MessageQueue都有个对象的ProcessQueue对象（一个 TreeMap 和一个读写锁 ）,TreeMap 以Offset为key 以消息内容为value。读写锁控制着多线程对 TreeMap的并发访问。
    - **流量控制：①消息未处理个数 1000 ;②消息总大小100M；③Offset跨度 2000；**

```
    public class ProcessQueue {
        private final ReadWriteLock lockTreeMap = new ReentrantReadWriteLock();
        //缓存的待消费消息,按照消息的起始offset排序
        private final TreeMap</*消息的起始offset*/Long, MessageExt> msgTreeMap = new TreeMap<Long, MessageExt>();
        //缓存的待消费消息数量
        private final AtomicLong msgCount = new AtomicLong();
        //缓存的待消费消息大小
        private final AtomicLong msgSize = new AtomicLong();
        ...
    }
```

- **DefaultMQPushConsumer  由系统控制读取操作，收到消息后自动调用用户线程的处理方法来处理。**
    - **消费者流控；每次拉取之前，都进行流控检查，不通过延迟50ms重试拉取。拉取的消息，都放在messageQueue缓存。**
    - 应用程序对消息的拉取过程参与度不高，可控性不足，仅仅提供消息监听器的实现。

- **DefaultMQPullConsumer  读取操作中的大部分功能由使用者自主控制**，要注意Offset的保存与同步。发送到broker的提交位移永远都是0，所以broker无法记录有效位移，**需要程序自己记录和控制提交位移**。
    - 应用程序对消息的拉取过程参与度高，由可控性高，可以自主决定何时进行消息拉取，从什么位置offset拉取消息

```
DefaultMQPullConsumer consumer = new DefaultMQPullConsumer("please_rename_unique_group_name");  
consumer.setNamesrvAddr("localhost:9876");  
consumer.start();  
  
// 假设你有一个方法来获取存储的offset，这里用0作为示例  
long offset = 0;  
  
try {  
    MessageQueue mq = new MessageQueue("TopicTest", "BrokerName", 0);  
    PullResult pullResult = consumer.pullBlockIfNotFound(mq, "*", offset, 32);  
      
    switch (pullResult.getPullStatus()) {  
        case FOUND:  
            List<MessageExt> msgFoundList = pullResult.getMsgFoundList();  
            // 处理消息...  
              
            // 更新offset，这里假设每条消息都成功消费，将offset更新为下一条消息的offset  
            if (!msgFoundList.isEmpty()) {  
                MessageExt lastMsg = msgFoundList.get(msgFoundList.size() - 1);  
                offset = lastMsg.getQueueOffset() + 1;  
                  
                // 存储新的offset，这里用打印作为示例  
                System.out.println("Updated offset to: " + offset);  
                // 实际情况下，你需要将这个offset存储到数据库或本地文件  
            }  
            break;  
        case NO_MATCHED_MSG:  
            break;  
        case NO_NEW_MSG:  
            break;  
        case OFFSET_ILLEGAL:  
            break;  
        default:  
            break;  
    }  
} catch (Exception e) {  
    e.printStackTrace();  
}  
  
consumer.shutdown();

```





# 7. 多线程、多队列如何并发消费

- **队列消费原则: 一个消费者可以消费多个消息队列，但一个消息队列同一时间只能被一个消费者消费.**

- **BROADCASTING：广播模式，分配 Topic 对应的所有消息队列。CLUSTERING 集群模式 ,topic与队列按照负载算法分配。**
- 每一个消费应用会存在多个topic的消费组consumer,每个consumer都可以配置多个线程消费。
- **应用启动后，RebalanceService服务每waitInterval=20s秒进行doRebalance()重新负载，获取topic对应的topic与消费队列。**
- 然后将新的消费队列对应的拉去请求PullRequest 放入 PullMessageService.pullRequestQueue队列中。

```
public class PullMessageService extends ServiceThread {
    private final InternalLogger log = ClientLogger.getLog();
   //当前拉取请求队列
    private final LinkedBlockingQueue<PullRequest> pullRequestQueue = new LinkedBlockingQueue<PullRequest>();
    private final MQClientInstance mQClientFactory;
   
   // ... 略
   
}
```

- PullMessageService类中也对应定时器，拉取队列消息。 参考 DefaultMQPushConsumerImpl.pullMessage方法实现。
    - 如果**消费者状态非Running、暂停、流量控制、异常等都会进入延迟执行**  this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL) 。延迟执行逻辑还是将PullRequest 放入 PullMessageService.pullRequestQueue队列中。
    - **消费者进度大于2000**：出现这种情况也是非常有可能的，其主要原因就是消费偏移量为 100 的这个线程由于某种情况卡主了（“阻塞”了），其他消息却能正常消费，这种情况虽然不会造成内存溢出，但大概率会造成大量消息重复消费，究其原因与消息消费进度的提交机制有关，在 RocketMQ 中，例如消息偏移量为 2001 的消息消费成功后，向服务端汇报消费进度时并不是报告 2001，而是取处理队列中最小偏移量 100，这样虽然消息一直在处理，但消息消费进度始终无法向前推进，试想一下如果此时最大的消息偏移量为 1000，项目组发现出现了消息积压，然后重启消费端，那消息就会从 100 开始重新消费，会造成大量消息重复消费，RocketMQ 为了避免出现大量消息重复消费，故对此种情况会对其进行限制，超过 2000 就不再拉取消息了。
    - 自动清理过期消息：ConsumeMessageConcurrentlyService无序消费时，会启动15分钟一次定时清理消息的任务。消费者它们会周期性地检查消息队列中的消息，根据消息的存储时间或过期策略，自动移除那些已经过期且不再需要保留的消息。这有助于释放存储空间，保持消息队列的紧凑和高效运行。


![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211118204743940-2077611872.png)


![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211118205035244-1825926438.png)

- 调用PullAPIWrapper对拉去结果状态进行判断处理
    - 有新消息(FOUND)
        - **设置PullRequest下次开始消费的起始位置为PullResult的nextBeginOffset**
        - 如果结果列表为空则不延迟，立马放到PullMessageService的待处理队列中
        - 将PullResult中的结果List<MessageExt>放入ProcessQueue的缓存中，并通知ConsumeMessageService处理
        - 将该PullRequest放回待处理队列中等待再次处理，如果有设置拉取的间隔时间，则等待该时间后再翻到队列中等待处理，否则直接放到队列中等待处理
    - 没有新消息(NO_NEW_MSG)
        - 设置PullRequest下次开始消费的起始位置为PullResult的nextBeginOffset
        - 如果缓存的待消费消息数为0，则更新本地内存offset存储。
        - 将PullRequest立马放到PullMessageService的待处理队列中
    - 没有匹配的消息(NO_MATCHED_MSG)
        - 设置PullRequest下次开始消费的起始位置为PullResult的nextBeginOffset
        - 如果缓存的待消费消息数为0，则更新本地内存offset存储。
        - 将PullRequest立马放到PullMessageService的待处理队列中
    - 不合法的偏移量(OFFSET_ILLEGAL)
        - 设置PullRequest下次开始消费的起始位置为PullResult的nextBeginOffset
        - 标记该PullRequset为drop
        - 10s后再更新并持久化消费offset；再通知Rebalance移除该MessageQueue


- **每次从消费进度拉取32个队列消息拉取成功后，将消息按照消费数(默认为1)分批放入消费者线程池中。**

```
public void submitConsumeRequest(final List<MessageExt> msgs,final ProcessQueue processQueue,final MessageQueue messageQueue,final boolean dispatchToConsume) {
   //用户消费队列数 默认1。 msgs的size 默认是32 
   final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize();
   if (msgs.size() <= consumeBatchSize) {
      ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue);
      try {
         //消费者开启的线程池
         this.consumeExecutor.submit(consumeRequest);
      } catch (RejectedExecutionException e) {
         this.submitConsumeRequestLater(consumeRequest);
      }
   } else {
      for (int total = 0; total < msgs.size(); ) {
         List<MessageExt> msgThis = new ArrayList<MessageExt>(consumeBatchSize);
         for (int i = 0; i < consumeBatchSize; i++, total++) {
            if (total < msgs.size()) {
               msgThis.add(msgs.get(total));
            } else {
               break;
            }
         }

         ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue);
         try {
            this.consumeExecutor.submit(consumeRequest);
         } catch (RejectedExecutionException e) {
            for (; total < msgs.size(); total++) {
               msgThis.add(msgs.get(total));
            }

            this.submitConsumeRequestLater(consumeRequest);
         }
      }
   }
}
```

# 8. 消息消费进度如何保存？

- 消息消费完成后，需要将消费进度存储起来，即前面提到的offset。
    - 广播模式下，同消费组的消费者相互独立，消费进度要单独存储,消费失败不支持重试；
    - 集群模式下，同一条消息只会被同一个消费组消费一次，消费进度会参与到负载均衡中，故消费进度是需要共享的。
    - 消费进度相关类 OffsetStore：
        - ① LocalFileOffsetStore 本地存储消费进度的具体实现，给广播模式使用；
        - ② RemoteBrokerOffsetStore 给集群模式使用，将消费进度存储在broker。

- 消息消费完成后，将消息从ProcessQueue中移除，同时返回ProcessQueue中最小的offset，使用这个offset值更新消费进度，removeMessage返回的offset有两种情况，一是**已经没有消息，返回ProcessQueue最大offset+1；二是还有消息，则返回未消费消息的最小offset。**
    - 举个例子，ProcessQueue中有offset为101-110的10条消息，如果全部消费完了，返回的offset为111；如果101未消费完成，102-110消费完成，则返回的offset为101，这种情况下如果消费者异常退出，会出现重复消费的风险，所以要求消费逻辑幂等。

- **在每次拉取后返回不合法的偏移量OFFSET_ILLEGAL时，则更新offset并持久化到远程Broker。**

- 在MQClientInstance启动的时候会注册定时任务，**每5s执行一次persistAllConsumerOffset()，最终调用到persistAll().会将消费进度持久到远程Broker。**

- **应用关闭时，也会将消费进度同步到远程。**

- offset读取：内存读,读取不到再远程Broker读。

# 9. RocketMQ再平衡触发条件

集群模式下，当消费者实例数量发生变化（如新增或减少消费者实例），会触发Rebalance，导致部分消息被重新分配并消费。再平衡操作并不会同步地在所有消费者上执行，而是每个消费者本地触发。

- **心跳检测周期** 消费者会与Broker建立长连接，定期30s发送心跳。如果再规定的时间内没有收到消费者心跳，则认为消费者已死亡。消费者与消息队列（Message Queue）的关系主要保存在 Broker 中。
  - Broker会向所有的在线消费者回复心跳信息，告知当前消费组与成员状态。如果消费实例数有变动，其他在线消费者收到后会触发再平衡。
- **消费者定时再平衡** 消费者的20s/次定时再平衡。

# 10. RocketMQ哪些场景重复消费

- **消费者程序异常,消息重试**：
  当消费者在处理消息时抛出未捕获的异常，RocketMQ认为该消息没有被正常消费，因此会尝试重新投递该消息，可能导致消息被多次处理。
- **消费者主动ACK机制失效**：
  在手动ACK模式下，如果消费者正确处理了消息但未能成功发送ACK确认给RocketMQ，RocketMQ会认为消息未被消费，从而再次投递。
- **消费者实例重启或网络闪断**：
  当消费者实例重启或因网络问题与RocketMQ短暂失去连接，RocketMQ可能会认为消费者已消费的消息实际上未被处理，从而重新分配这些消息。
- **Consumer Group内成员变动**：
  在CLUSTERING模式下，如果Consumer Group内的消费者实例数量发生变化（如新增或减少消费者实例），会触发Rebalance，可能导致部分消息被重新分配并消费。
- **Broker故障恢复或数据迁移**：
  Broker端的问题，如故障恢复或数据迁移过程中，可能会导致消费者收到重复消息。
- **生产者消息重投**：
  生产者在发送消息时，同步消息失败会重投，默认3次；异步无重试有回调函数；oneway没有任何保证。重投可能会造成消息重复。

所以，消费者确保消息处理过程是幂等的。

# 11. RocketMQ消息Tag过滤

![](https://img2024.cnblogs.com/blog/1694759/202407/1694759-20240729190558086-1442480046.png)

- Rocket MQ 消息过滤包括基于表达式与基于类模式的两种过滤方式，其中表达式又分为 tag 和 sql92 模式（sql92 模式可以会对用户属性字段进行复杂的过滤），且都是在**服务端**对消息进行过滤。

- 基于tag过滤还会在客户端再进行一次过滤：
    - 由于在消息服务端进行消息过滤是匹配消息tag的hashcode，导致服务端过滤并不十分准确。

# 12. RocketMQ最大消息大小

RocketMQ默认最大消息大小通常是 4 MB。调整最大消息大小注意：

- broker 端调整 maxMessageSize
- 生产者端已通过 setMaxMessageSize 方法设置了更大的消息大小

# 13. RocketMQ顺序消费


- **全局顺序消息**：只有一个队列，性能较差；
- **分区顺序消息**：多个队列,单一队列中消息顺序。生产者采用按key取模运算，将同一key消息都放到同一个队列。消费端要使用MessageListenerOrderly接口。
  - 消息消费失败，无法跳过，每次重试的间隔时间为1秒，当前队列消费暂停，性能也受影响。当超过最大重试次数16次时，进入死信队列。

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211119134643158-1229659813.png)

- RocketMQ消息消费模式如下图：

![](https://img2024.cnblogs.com/blog/1694759/202406/1694759-20240607141854626-811717059.png)

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211119134255577-809940184.png)

- 1.第一把锁，Broker中在类RebalanceLockManager的静态变量 mqLockTable (变量类型为 ConcurrentMap)中存储了以消费组 为key ,以 ConcurrentMap (以消息主题，主题下队列为key，具体信息是消费者客户端id 为和客户端上次锁定时间 为value的 LockEntity 对象）为value的消费者锁定信息;
- 2.第二把锁分布式锁，broker接受请求后执行RebalanceLockManager 的 tryLockBatch方法。保证同一个consumerGroup下同一个messageQueue只会被分配给一个consumerClient。执行顺序如下：
    + 1.请求参数解析，解析成 要锁定的主题下队列集合和消费者ID；
    + 2.遍历请求锁定的队列
    + 3.通过 mqLockTable 判断单个队列是否已经锁定,即调用 LockEntry 的 isLocked 方法,主要是判断 clientId 是否是当前消费者ID,如果是就更新锁定时间，并加入已经锁定队列中,如果 mqLockTable 不存在 这个消费组或者当前锁定的clientId与请求的clientId 不相等，就加入未锁定队列;
    + 4.判断未锁定队列是否为空，不为空,判断当前消费组是否在mqLockTable 中,不存在就创建,后启用 RebalanceLockManager的可重入锁，遍历未锁定队列.
    + 5.执行第3步
    + 6.释放 RebalanceLockManager 的可重入锁，返回当前锁定的信息。
- 3.第三把锁分布式锁，consumer上顺序消费的类有个定时任务，每隔20秒去向broke发送它订阅的topic的锁定队列请求。目的:快速适应动态调整与再平衡、锁的有效性检查与刷新。
- 4.第四把锁，consumer上在获取到队列的消息的时候，让消费线程池去处理，处理前必须获取到本地队列的锁。参考：ConsumeMessageOrderlyService.ConsumeRequest 类。

# 14. 事务消息(半消息):

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211116145454842-308649345.png)

![](https://img2024.cnblogs.com/blog/1694759/202406/1694759-20240614141921876-1836304886.png)

![](https://img2020.cnblogs.com/blog/1694759/202109/1694759-20210930143000830-2069655612.png)

RocketMQ事务消息的实现原理是类似**基于二阶段提交与事务状态回查**来实现的。事务消息的发送只支持同步方式，其实现的关键点包括：

+ 在应用程序端，在一个本地事务中，通过发送消息API向Broker发送Prepare状态的消息，收到消息服务器返回成功后执行事件回调函数，在事件函数的职责就是记录该消息的事务状态，通常采用**消息发送本地事务表**，即往本地事务表中插入一条记录，如果业务处理成功，则消息本地事务中会存在相关记录；如果本地事务执行失败而导致事务回滚，此时本地事务状态中不存在该消息的事务状态。
+ Broker收到Prepare的消息时，如何保证消息不会被消费端立即处理呢？原来消息服务端收到Prepare状态的消息，会先备份原消息的主题与队列，然后变更主题为：**RMQ_SYS_TRANS_OP_HALF_TOPIC**，队列为0。
+ Broker会开启一个专门的线程，以每60s的频率从RMQ_SYS_TRANS_OP_HALF_TOPIC中拉取一批消息，进行事务状态的回查，其实现原理是根据消息所属的消息生产者组名随机获取一个生产者，向其询问该消息对应的本地事务是否成功，如果本地事务成功(该部分是由业务提供的事务回查监听器来实现)，则消息服务端执行提交动作；如果事务状态返回失败，则消息服务端执行回滚动作；如果事务状态未知，则不做处理，待下一次定时任务触发再检查。默认如果连续5次回查都无法得到确切的事务状态，则执行回滚动作。


# 15. RocketMQ消费特殊的Topic

- 1.延迟消息（18次：1s、5s、10s、30s、1m ... 2h ）(**特殊的Topic**)
    - 定时消息会暂存在名为**SCHEDULE_TOPIC_XXXX**(固定名称的，XXXX不是占位符)的topic中，并根据delayTimeLevel存入特定的queue,即一个queue只存相同延迟的消息.broker会定时调度地消费SCHEDULE_TOPIC_XXXX，将消息写入真实的topic。

- 事务消息(**特殊的Topic**)
    - broker服务端收到Prepare状态的消息，先备份原消息的主题与队列，然后变更Topic为**RMQ_SYS_TRANS_OP_HALF_TOPIC**，队列为0。
    - 待事务完成后，再将消息移动到将消息写入真实的topic中。

- 重试队列（16次：10s、30s、1m ... 2h ）(**特殊的Topic**)
    - 对于顺序消息，当消费者消费消息失败后，消息队列 RocketMQ 会自动不断进行消息重试（每次间隔时间为 1 秒），这时应用会出现消息消费被阻塞的情况。
    - 对于非顺序消息，消费失败后，客户端主动将msg和重试次数信息上报给Broker，Broker收到消息后，会将消息移动到重试队列中。
    - 消费重试的时间间隔与延时消费的延时等级十分相似,Broker对于重试消息的处理是通过延时消息实现的,先将重试队列中，延迟时间到后，会将消息投递真实的topic中。
    - 重试队列是针对消费组的，而不是针对每个Topic设置，每个消费组都会创建特殊的Topic，**%RETRY%+consumerGroup**.


- 死信队列(**特殊的Topic**)
    - **就是一个特殊的Topic**，名称为%DLQ%consumerGroup@consumerGroup，每个**消费组**都有的，如果重试消息达到最大后进入死信队列。未产生死信消息时，不会产生。

# 16. RocketMQ的消息堆积如何处理

1.增大消费者线程数;
2.如果Queue>消费者数量，增消费者实例;
3.如果Queue<消费者数量的情况。可以使用准备一个临时的 topic，同时创建一些 queue，在临时创建一个消费者来把这些消息转移到 topic 中,让消费者消费。



# 20. RocketMQ保存的数据结构

RocketMQ 主要的存储文件包括 CommitLog 文件、ConsumeQueue 文件、Indexfile索引 文件。

- **CommitLog：存储 Producer 端写入的消息主体内容**，消息内容不是定长的。**Broker内所有topic共享一个数据文件CommitLog日志文件**。单个文件大小默认 1G, 文件名长度为 20 位，左边补零，剩余为起始偏移量，第一个文件写满了，第二个文件起始偏移量为 1073741824，以此类推。

- **ConsumeQueue文件作为消费消息的索引，每个主题（Topic）都有一个对应的ConsumeQueue组件， 用于快速定位消息的位置，提高消息的消费效率。** 由多个逻辑队列（Logical Queue）组成，每个逻辑队列对应一个消息队列（Message Queue）。逻辑队列中存储了消息的索引信息，包括消息在CommitLog文件中的偏移量（offset）和消息的大小。

- **索引文件（Index File）**：索引文件用于记录每个消息队列的消息索引信息。它包含了每个消息的偏移量和消息的存储时间戳。按照消息的存储时间戳进行排序，提供**通过 key 或时间区间来查询消息**的方法。

- 位图文件（BitMap File）：位图文件用于记录消息队列中每个消息的消费状态。每个消息占用一个位，0表示未消费，1表示已消费。通过位图文件，可以快速判断消息是否已经被消费。

ConsumeQueue中的消息格式如下：

```
+---------------------+---------------------+
|       Offset        |     CommitLogOffset |
|       (8字节)       |       (8字节)        |
+---------------------+---------------------+
|      Size           |     TagsCode        |
|      (4字节)        |     (8字节)          |
+---------------------+---------------------+
|      StoreTimestamp |    ConsumeTimestamp |
|      (8字节)        |     (8字节)          |
+---------------------+---------------------+
```

* Offset：消息在ConsumeQueue文件中的偏移量。
* CommitLogOffset：消息在CommitLog文件中的物理偏移量。
* Size：消息的大小。
* TagsCode：消息的标签编码。
* StoreTimestamp：消息的存储时间戳。
* ConsumeTimestamp：消息的消费时间戳。


# 22. RocketMQ如何保证消息不丢失

- Producer端：同步发送,多个Broker重试，默认3次。异常后业务可重试。
- Broker端:  修改刷盘策略为同步刷盘，持久化到本地磁盘。默认情况下是异步刷盘的,集群部署。消息会被写入消息日志文件（Commit Log）和索引文件（Index File）。
- 主从同步：消息在主节点持久化后，会通过主从同步机制将消息复制到从节点。这样即使主节点宕机，消息仍然可以在从节点上访问。
- Consumer端: 完全消费正常后在进行ack确认，Broker会记录消费进度。异常会进入消费重试，如果消息多次重试扔消费不成功，会进入死信队列。

# 23. RocketMQ如何保证高可用

高可用是通过Broker的集群和主从实现的，多主多从模式（同步双写,异步刷盘）

![](https://img2024.cnblogs.com/blog/1694759/202407/1694759-20240729192611381-909386785.png)

- 消费者：当Master不可用或者繁忙的时候， Consumer 的读请求会被自动切换到从 Slave，不影响 Consumer 读取消息。
- 生产者：在创建Topic 的时候，把Topic的多个Message Queue创建在多个Broker组上，当某个Broker组master不可用时，只会影响对应Broker组内的Message Queue的写请求，其他组master仍然可写。生产者选择队列时，有一个 sendLatencyFaultEnable 开关变量，如果开启，在随机递增取模的基础上，再过滤掉 not available 的 Broker 代理。

# 25. RocketMQ常用部署模式

单机模式
多主模式
双主双从/多主多从模式（异步复制,同步刷盘）,故障手动处理。性能略高于同步双写。但是会存在数据丢失风险。
**双主双从/多主多从模式（同步双写,异步刷盘）**,故障手动处理。从节点也可以无缝接管，进一步增强了系统的高可用性。
Dledger 集群模式: 4.5版本之后,采用Raft协议,一主多从,故障可自动转移。

# 26. 主从同步(HA)

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211116145413996-648336471.png)

A.首先启动Master并在指定端口监听；
B.客户端启动，主动连接Master，建立TCP连接；
C.客户端以每隔5s的间隔时间向服务端拉取消息，如果是第一次拉取的话，先获取本地commitlog文件中最大的偏移量，以该偏移量向服务端拉取消息；
D.服务端解析请求，并返回一批数据给客户端；
E.客户端收到一批消息后，将消息写入本地commitlog文件中，然后向Master汇报拉取进度，并更新下一次待拉取偏移量；
F.然后重复第3步；



# 40. RocketMQ与Kafka区别

- 1.架构区别

    + RocketMQ由NameServer、Broker、Consumer、Producer组成，NameServer之间互不通信，Broker会向所有的nameServer注册，通过心跳判断broker是否存活，producer和consumer 通过nameserver就知道broker上有哪些topic。
    + Kafka的元数据信息都是保存在Zookeeper，新版本部分已经存放到了Kafka内部了，由Broker、Zookeeper、Producer、Consumer组成。
    + 两者都支持事务消息（ Kafka从0.11.0.0 版本）、顺序消息。

- 2.维度区别

    + Kafka的master/slave是基于partition(分区)维度的，而RocketMQ是基于Broker维度的；
    + Kafka的master/slave是可以切换的（主要依靠于Zookeeper的主备切换机制）
    + RocketMQ无法实现自动切换，当RocketMQ的Master宕机时，读能被路由到slave上，但写会被路由到此topic的其他Broker上。在4.5之后的版本有**DLedger多副本机制，可以自动故障切换**。

- 3.刷盘机制

    + RocketMQ默认采用的异步刷盘，。Kafka默认是异步刷盘,也支持同步刷盘。
        + 同步刷盘：也每次消息都等刷入磁盘后再返回，保证消息不丢失，但对吞吐量稍有影响。一般在主从结构下，选择同步双写异步刷盘策略是比较可靠的选择。

- 4.消息查询
    + RocketMQ支持消息查询，除了queue的offset外，还支持自定义key。RocketMQ对offset和key都做了索引，均是独立的索引文件。

- 5.服务治理

    + Kafka用Zookeeper来做服务发现和治理，broker和consumer都会向其注册自身信息，同时watch机制订阅相应的znode，这样当有broker或者consumer宕机时能立刻感知，做相应的调整；
    + RocketMQ用自定义的nameServer做服务发现和治理，其实时性差点，比如如果broker宕机，producer和consumer不会实时感知到，需要等到下次更新broker集群时(最长30S)才能做相应调整，服务有个不可用的窗口期，但数据不会丢失，且能保证一致性。但是某个consumer宕机，broker会实时反馈给其他consumer，立即触发负载均衡，这样能一定程度上保证消息消费的实时性。

- 6.消费确认

    + RocketMQ定期向broker同步消费进度(每5s)，offset存在broker上。
    + Kafka支持定时确认、拉取到消息自动确认、手动确认，offset存在zookeeper上。

- 7.消息回溯

    + Kafka理论上可以按照Offset来回溯消息。
    + **RocketMQ支持按照Offset和时间来回溯消息，精度毫秒**，例如从一天之前的某时某分某秒开始重新消费消息，典型业务场景如consumer做订单分析，但是由于程序逻辑或者依赖的系统发生故障等原因，导致今天消费的消息全部无效，需要重新从昨天零点开始消费，那么以时间为起点的消息重放功能对于业务非常有帮助。

- 8.数据写入：

    - Kafka **partition是Topic物理上的分组，每个partition独占一个目录，每个partition均有各自的数据文件.log**；引入了日志分段(每段1GB),每个日志文件对应两个索引文件（偏移量索引文件、时间戳索引文件），提高消息查询效率。
        - Kafka默认使用异步发送的形式，有一个memory buffer暂存消息，同时会将多个消息整合成一个数据包发送，这样能提高吞吐量，但对消息的实效有些影响。
        - **Kafka的数据写入速度比RocketMQ高出一个量级**。但超过一定数量的文件同时写入，会导致原先的顺序写转为随机写，性能急剧下降，所以Kafka的分区数量是有限制的。
        - Kafka默认是异步刷盘，消息写入时存进memory buffer就返回。
    - RocketMQ是**每个Broker内所有topic共享一个数据文件CommitLog日志文件**，默认1G（新文件的文件名会根据起始偏移量进行命名），每个队列都有一个索引文件(偏移量和消息的存储时间戳).
        - RocketMQ可选择使用同步或者异步发送。
        - RocketMQ需要等broker的响应确认。
        - RocketMQ可以做到比Kafka支持更多队列
- 9.特有

    + RocketMQ支持tag、支持延时消息、消息重试机制、支持按时间和offset回溯
    + Kafka 引入了日志分段(每段1GB),每个日志文件对应两个索引文件（偏移量索引文件、时间戳索引文件），提高消息查询效率。

- 10.两者都高性能

    - **页缓存技术**、**磁盘顺序写**、**内存映射文件（mmap）**、**零Copy技术**
    - 都是10w+性能，kafka性能要高于RocketMQ，单机Kafka量级19w/s、RocketMQ 12w/s

- 11.消费并行度
    - Kafka主要通过pull模式进行消费，而RocketMQ支持pull和push两种模式。
    - **Kafka的消费者默认是单线程的，一个Consumer可以订阅一个或者多个Partition，一个Partition同一时间只能被一个消费者消费**，也就是有多少个Partition就最多有多少个线程同时消费
    - RocketMQ消费并行度分两种情况：有序消费模式(和Kafka一致)和并发消费模式(可以配置线程池，并发度比Kafka高出一个量级)，
    - **RocketMQ更适合多Topic，多消费端的业务场景； Kafka适合Topic和消费端都比较少的业务场景**

RocketMQ定位于非日志的可靠消息传输，目前RocketMQ在阿里集团被广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理，binglog分发等场景。
**RocketMQ的同步刷盘在单机可靠性上比Kafka更高，不会因为操作系统Crash，导致数据丢失。**
数据同步Replication也比Kafka异步Replication更可靠，数据完全无单点。
另外Kafka的Replication以topic为单位，支持主机宕机，备机自动切换，但是这里有个问题，由于是异步Replication，那么切换后会有数据丢失，同时Leader如果重启后，会与已经存在的Leader产生数据冲突。

# 41. Kafka最大消息大小

Kafka服务器默认最大消息大小通常是 1 MB。调整最大消息大小注意：

- broker 端调整 message.max.bytes，
- 生产者端调整 max.request.size
- 消费者端调整 max.partition.fetch.bytes，确保这些值都适当地设置以允许更大消息的传输，并且要保证生产者的配置不超过 broker 端的限制。
- 副本同步message.max.bytes 的值必须小于等于 replica.fetch.max.bytes。

# 42. Kafka为什么不支持读写分离

Leader/Follower 模型并没有规定 Follower 副本不可以对外提供读服务。很多框架都是允许这么做的，只是 Kafka 最初为了避免不一致性的问题，而采用了让 Leader 统一提供服 务的方式。Kafka 2.4 之后，Kafka 提供了有限度的读写分离，也就是说Follower副本能够对外提供读服务。

> HW和LEO。HW俗称高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。
> 对于leader新写入的消息，consumer不能立刻消费，leader会等待该消息被所有ISR中的replicas同步后更新HW，

之前的版本不支持读写分离的理由。

- 场景不适用。读写分离适用于那种读负载很大，而写操作相对不频繁的场景，可 Kafka 不属于这样的场景。
- 同步机制。Kafka 采用 PULL 方式实现 Follower 的同步，因此，Follower 与 Leader 存 在不一致性窗口。如果允许读 Follower 副本，就势必要处理消息滞后(Lagging)的问题。



# 43. Kafka的ack机制

request.required.acks 有三个值 0、1、 -1(ALL)

- 0:  生产者不会等待broker的ack，消息存进memory buffer就返回（0）。这个延迟最低但是存储的保证最弱当 server 挂掉的时候就会丢数据
- 1:  服务端会等待ack值， 半数以上副本(ISR)确认接收到消息后发送ack。但是如果leader挂掉后，不确保是否复制完成，新leader也会导致数据丢失。
- -1(ALL) : 同样在 1 的基础上 服务端会等所有的follower的副本受到数据后才会受到leader发出的ack，这样数据不会丢失

# 44. Kafka如何实现百万级并发写入

- **页缓存技术**： 每次接收到数据直接写入OSCache中
- **磁盘顺序写**：每次都是追加文件末尾顺序写的方式.
    - 刷盘策略：
        - acks=-1 配置表示生产者只有在所有副本确认收到消息后才认为消息发送成功，这增加了消息的持久性。
        - acks=1 ISR（In-Sync Replica）列表中的大多数副本时，才被认为是已提交（committed）。
        - log.flush.interval.messages和log.flush.interval.ms控制了数据多久或者积累多少条消息后刷盘，减少数据在内存中停留的时间，降低数据丢失风险。
- **零Copy**： 堆外内存，直接让操作系统的 Cache 中的数据发送到网卡后传输给下游的消费者。跳过数据Copy应用内存。
- **mmap内存映射文件**：将文件映射到内存地址空间，程序可以直接按内存访问的方式读写文件内容，而无需传统的文件读写调用。减少了数据复制的开销。
    - RocketMQ在处理消息存储时，需要对磁盘上的CommitLog、ConsumeQueue等文件进行频繁的读写操作。使用mmap技术，可以将这些文件映射到进程的虚拟内存空间，从而避免传统的read/write操作带来的多次数据拷贝。

# 45. Kafka控制器的作用

Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。
当某个分区的 leader 副本出现故障时，由控制器负责为该分区选举新的 leader 副本。
当检测到某个分区的 ISR 集合发生变化时，由控制器负责通知所有broker更新其元数据信息。
