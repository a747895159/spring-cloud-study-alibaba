https://www.cnblogs.com/crazymakercircle/p/14285001.html

[TOC]

# 1. 服务雪崩

- 多个微服务调用,比如服务A调用服务B、服务B又调用服务C、服务C又调用其他... 就是**扇出**。
- 当扇出的链路上某个服务调用时间过长或不可用时，微服务A的调用就会占用越来越多的资源，进而引起系统崩溃。
- 也同时会引起其他接口服务调用延迟，系统资源紧张，发生级联故障。
  此时需要需要服务链路做熔断、降级方案，防止雪崩。

# 2. 服务熔断、降级、隔离、限流的理解

+ **降级**：根据实际业务情况，从资源保护和系统负载均衡的角度出发，客户端（调用方）对一些服务和页面进行有策略的不处理。
	- **超时降级**： 根据超时响应时间与重试机制，自动返回服务预期的响应(FallBack)
	- **失败次数降级**：对一些API调用，当失败次数或失败率达到一定的阈值时
	- **故障降级**：要调用的远程服务挂了，直接返回一些兜底处理方案(使用之前缓存数据等)
	- **限流降级**：对一些可预见的访问量大接口，提前做好限流配置，保护系统稳定。当超过限流阈值，直接返回错误提示。
	- **主要是由客户端设置,一般由外围业务开始,牺牲非核心服务，保证核心服务稳定；从整体负荷考虑**

+ **熔断**：是一种框架级别的保护机制，是对降级兜底的技术实现，当客户端（调用方）对服务方的请求和调用出现大量失败和超时,这时熔断该服务的所有调用,从而快速释放资源。
	- 熔断器状态：
		- **Closed**：熔断器关闭状态，调用失败次数积累，到了阈值（或一定比例）则启动熔断机制；
		- **Open**：熔断器打开状态，此时对下游的调用都内部直接返回错误，不走网络，但设计了一个时钟选项，默认的时钟达到了一定时间（这个时间一般设置成平均故障处理时间，也就是MTTR），到了这个时间，进入半熔断状态；
		- **Half-Open**：半熔断状态，允许定量的服务请求，如果调用都成功（或一定比例）则认为恢复了，关闭熔断器，否则认为还没好，又回到熔断器打开状态；
+ **隔离**：服务方为保护整体资源不被某些接口资源耗尽，这些接口配置最大可以使用的资源。
+ **限流**：对一些可预见的访问量大接口，提前做好限流配置，保护系统稳定。支持多种维度控制(请求参数、IP、Header等)。**隔离在一定维度上也是限流**。

+ 降级、熔断是对客户端（调用方）的保护。隔离、限流是对服务方的保护。

# 3. 两种资源隔离方式

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210823201516541-1750466451.png)

- 线程池隔离
	- 优点:
		- 控制并发：通过线程池大小限制了对特定服务的并发调用数，有助于防止服务过载。
		- 非阻塞操作：不会占用 Tomcat 的工作线程。
	- 缺点:
		- 资源开销：每个服务都需要维护一个线程池，增加了内存消耗和管理成本。
		- 上下文切换：线程间的上下文切换会带来额外的性能开销。
		- 过度隔离：对于轻量级服务，使用线程池可能过于重，导致资源浪费。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210823201306949-2140770679.png)

- 信号量隔离
	- 优点:
		- 轻量级：通过计数器限制并发数，实现简单。不涉及线程的创建和销毁，减少了资源开销和上下文切换的开销。
		- 高效：适用于非阻塞、快速完成的操作，能够提供更高的吞吐量。
	- 缺点:
		- 阻塞操作：Tomcat 工作线程被阻塞，影响整体响应能力。
		- 资源利用不足：对于快速完成的任务，信号量可能过早地拒绝请求，即使系统有能力处理更多并发。
		- 缺乏灵活性：相较于线程池，信号量在处理复杂场景和提供高级特性（如超时、降级策略）方面较为有限。

> 示例：A 调用B、C、D时，当BCD之间没有逻辑依赖关系。
> 1.在B做线程池隔离时，不阻塞Tomcat工作线程，会继续调用C/D
> 2.在B做信号量隔离时，阻塞Tomcat工作线程，需等待B处理完。


# 4. 限流算法原理

+ 1.固定窗口限流算法：

	- 定义：单位窗口时间内当次数少于限流阈值,就允许访问。否则拒绝。当前时间窗口过去后，计数器清零。
	- 缺点：临界问题,算法存在不精确。比如：1s内可以访问5次,在0.8-1s内访问5次， 而在 1s-1.2s也访问5次。这样就存在超过1s内访问5次的限制。

+ 2.滑动窗口限流算法:

	- 解决固定窗口临界值问题,将单位时间周期分为N个周期,分别记录每个小周期方位次数,根据滑动时间删除过期的小周期
	- 滑动窗口周期划分越多,那么滑动窗口的滚动就越平滑,限流统计越精确。
	- 固定窗口与滑动窗口都存在暴力拒绝,会损失一部分请求,不友好。

+ 3.漏桶算法：

	- 往漏桶中以任意速率流入水，以固定的速率流出水。当水超过桶的容量时，会被溢出，也就是被丢弃
	- 系统按照固定的速率处理请求，是我们想要的。但是面对突发流量的时候，漏桶算法处理不算友好。

+ 4.令牌桶算法：

	- 以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。
	- 除了要求能够限制数据的平均传输速率外，还要求允许某种程度的**突发传输**。这时候漏桶算法可能就不合适了，令牌桶算法更为适合
	- 示例：Guava的RateLimiter就是令牌桶算法。
	- 使用场景：
		- 漏桶算法：恒定速率流出，不支持突发流量。用户防止打垮我们依赖的服务，适用于因第三方服务固定速率限流的
		- 令牌桶算法：适用于我们自己维护的服务,面对突发情况,为保证系统最大可用性,防止服务被打垮。

  ![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210823173621224-1814502732.png)


# 5. Hystrix 与 Sentinel 的区别

都是用于构建高可用微服务架构的容错库，它们的核心功能确实包括服务熔断降级、和限流隔离策略。

### 相同点：

- 都支持限流、熔断降级（响应时间、失败比率），达到快速失败。
- 都是基于滑动窗口实现，支持多种数据源配置、扩展好，支持注解方式引入。

### 不同点
- Sentinel的线程隔离：‌Sentinel是基于信号量（‌计数器）‌实现的线程隔离，‌不需要创建线程池，‌因此具有较好的性能。‌
- Hystrix的线程池隔离：‌Hystrix默认是基于线程池实现的线程隔离，‌每个被隔离的业务都会创建一个独立的线程池。‌这种方式隔离性较强，‌但线程过多会带来额外的CPU开销。

- Sentinel 在功能上更加全面，支持限流，友好的控制台配置页面、可以展示各种配置与监控。社区维护活跃。Sentinel v1.8 文档手册](https://www.bookstack.cn/read/Sentinel-1.8/2ed1babeee77ea56.md)

	- 集群流控：分布式微服务对某个API限制总QPS。
	- 网关流控：根据限流规则，对用户自定义(请求中的参数、Header、来源 IP)的分组进行限流.
	- 热点参数限流：某个热点数据中访问频次最高的 Top K 数据，并对其访问进行限制
	- 系统自适应限流： 应用的 Load、总体平均 RT、入口 QPS 和线程数等几个维度的监控指标，让系统的入口流量和系统的负载达到一个平衡
	- 黑白名单限制：根据请求来源(origin)限制
	- 基于配置中心自定义动态规则：将控制规则保存在配置中心（Nacos、Apollo、Zookeeper、Redis ），以 **推模式** 实时获取动态规则。

- Hystrix 主要关注的是通过线程池和信号量机制来实现服务的资源隔离和熔断。限流方面支持比较薄弱，2019年进入维护模式，停止新功能开发，不支持各维度的限流，限流能力较弱。


| 对比项目       | Sentinel                             | Hystrix       |
| -------------- |--------------------------------------|---------------|
| 隔离策略       | **默认信号量**/线程池(基于信号量，不单独创建线程池)        | 信号量/**默认线程池** |
| 熔断降级策略   | 基于响应时间、失败比率                          | 基于响应时间、失败比率   |
| 实时指标实现   | 滑动窗口                                 | 滑动窗口(RxJava)  |
| 扩展性         | 多个扩展点                                | 插件的形式         |
| 注解的支持     | 支持                                   | 支持            |
| 限流           | 基于QPS，支持调用关系，支持各维度（IP、参数、请求Header）流控 | 支持较弱          |
| 流量整形       | 支持慢启动，匀速器模式                          | 不支持           |
| 系统自适应保护 | 支持                                   | 不支持           |
| 控制台         | 可配置规则、秒级监控，限流图形界面化                   | 不完善，仅有限流图形界面化 |
| 活跃度         | 阿里维护，社区活跃度高                          | 2019年停止更新，仅维护 |


```
hystrix:
	propagate:
	  request-attribute:
		enabled: true
	threadpool:
		default:  #全局默认配置
		  coreSize: 10 	# 线程池核心线程数
		  maximumSize: 20  	# 线程池最大线程数
		  queueSizeRejectionThreshold：30 # 线程池队列大小，
		  allowMaximumSizeToDivergeFromCoreSize: true   # 线程池maximumSize最大线程数是否生效
		  keepAliveTimeMinutes：10  					  # 设置可空闲时间，单位为分钟
	command:
	  #全局默认配置, 单个服务 可以指定serviceId，接口配置，格式为： 类名#方法名（参数类型列表）
	  default:
		#线程隔离相关
		execution:
		  timeout:
			#是否给方法执行设置超时时间，默认为true。一般我们不要改。
			enabled: true
		  isolation:
			#配置请求隔离的方式，这里是默认的线程池方式。还有一种信号量的方式semaphore，使用比较少。
			strategy: threadPool
			thread:
			  #方式执行的超时时间，默认为1000毫秒，在实际场景中需要根据情况设置
			  timeoutInMilliseconds: 10000
			  #发生超时时是否中断方法的执行，默认值为true。不要改。
			  interruptOnTimeout: true
			  #是否在方法执行被取消时中断方法，默认值为false。没有实际意义，默认就好！
			  interruptOnCancel: false
		circuitBreaker:   #熔断器相关配置
		  enabled: true   #是否启动熔断器，默认为true，false表示不要引入Hystrix。
		  requestVolumeThreshold: 20     #启用熔断器功能窗口时间内的最小请求数，假设我们设置的窗口时间为10秒，
		  sleepWindowInMilliseconds: 5000    #所以此配置的作用是指定熔断器打开后多长时间内允许一次请求尝试执行，官方默认配置为5秒。
		  errorThresholdPercentage: 50   #窗口时间内超过50%的请求失败后就会打开熔断器将后续请求快速失败掉,默认配置为50
```
```
//注解方式实现
@HystrixCommand(
    commandProperties = {
            @HystrixProperty(name = "execution.isolation.strategy", value = "THREAD")
    },
    threadPoolKey = "createOrderThreadPool",
    threadPoolProperties = {
            @HystrixProperty(name = "coreSize", value = "20"),
   @HystrixProperty(name = "maxQueueSize", value = "100"),
            @HystrixProperty(name = "maximumSize", value = "30"),
            @HystrixProperty(name = "queueSizeRejectionThreshold", value = "120")
    },
    fallbackMethod = "errMethod"
)

```


# 6. 应用级限流

+ 单实例应用级限流
	- SpringBoot 内嵌Tomcat,默认配置：最大工作线程数 200,最小工作线程数 10,最大连接数 10000, 当前连接数超过maxConnections时，还能接受的连接的数量（排队的数量） 100。
	- 使用Java 信号量
	- Hystrix
	- Guava RateLimiter提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。

+ 分布式限流
	- 业务上限流可以使用redis+lua或者nginx+lua技术进行实现
	- Sentinel基于QPS全服务限流

+ 接入层限流
	- Nginx流量控制
		- limit_conn用来对某个KEY对应的总的网络连接数进行限流，可以按照如IP、域名维度进行限流。
		- limit_req用来对某个KEY对应的请求的平均速率进行限流，并有两种用法：平滑模式（delay）和允许突发模式(nodelay)。



# 7. 设计限流组件，要考虑哪些因素

- 1.限流维度：http请求限流(配置在Nginx)、特定接口参数限流(需要接口层实现限流)
- 2.限流算法: 固定窗口法、漏斗法，以及令牌桶法，还有一些组件实现了具有预热功能的算法.
- 3.限流数据保存：Redisson 是保存在redis,Sentinel 保存在内存利用了Google开源的ConcurrentLinkedHashMap，利用它实现了LRU。
- 4.限流的数据量级：小数据量忽略、大数据量 要有一个存放空间已经过期策略。
- 5.时钟回拨或抖动,精细限流(毫秒级限流)就有问题。解决思路，重新获取时间。Redis-cell源码中就采用了这一方案。



# 11. CAP定理

**一致性**（Consistency）、**可用性**（Availability）、**分区容错性**（Partition tolerance）。分布式系统中，P是存在的，但是 一致性与可用性不能同时满足。
设计系统时采用BASE理论（基本可用、软状态、最终一致性），在大型分布式系统中通过牺牲强一致性，来获得可用性并允许数据在短时间内不一致，最终会达到一致状态。
例如：Zk 是CP模型、consul是CP模型、 Eureka是AP模型、Nacos是**CP(支持注册持久化实例)+AP(只支持注册临时实例)**模型

# 12. Zookeeper分布式锁

ZooKeeper是一个分布式应用程序协调服务，解决了分布式一致性问题，是集群的管理者提供**文件系统和通知机制**，提供 配置维护、集群管理、命名服务、分布式同步等功能。用于dubbo、Kafka框架的注册中心。
ZK分布式锁 通过临时顺序节点(最小节点)实现，每个客户端都去创建一个临时有序节点，然后获取到所有节点，判断自己是不是最小的节点，如果是则获得锁，否则监听自己前一个节点锁的释放。
适用于高可靠（高可用）而并发量不是太大的场景：因为每次在创建锁和释放锁的 过程中，都要动态创建、销毁临时节点来实现锁功能。

- 四种节点：持久节点、持久顺序节点、临时节点、临时顺序节点(连接建立、断开删除)
- 文件系统：树状的目录结构，不能存放大量数据，每个节点数据上限1M
- watch机制：一个Watch事件是一个一次性的触发器，当被设置了 Watch 的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端。注册watcher的方法：getData、exists、getChildren。
	- ZooKeeper的节点监听机制：顺序监听，按照临时节点顺序后面监听前面一个，能避免羊群效应。

![](https://img2024.cnblogs.com/blog/1694759/202406/1694759-20240614140639719-334406971.png)

Zookeeper 和 Etcd 相比于 Redis 实现分布锁，在功能层面有一个非常好用的特性：Watch。

![](https://img2024.cnblogs.com/blog/1694759/202408/1694759-20240826151922869-1024450777.png)


# 13. Zookeeper 工作原理

- Zookeeper 的核心是 原子广播 ，这个机制保证了 各个 Server 之间的同步 。实现这个机制的协议叫做 Zab 协议、paxos算法 。
  Zab 协议有两种模式，它们分别是 **恢复模式（选主）** 和 **广播模式（同步）** 。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。
  状态同步保证了 leader 和 Server 具有相同的系统状态。

- 服务器状态：LOOKING 寻找Leader（进入选主模式）、FOLLOWING 跟随者状态、LEADING 领导者状态、OBSERVING 观察者状态。
- ![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210806100750320-1903700679.png)
- 集群角色：leader、follower、observer(观察者，只读不写、不参与投票选举。扩展系统了，提高读速度)
- zk集群 至少需要一半机器上可用, 即至少3台。主要是为了选举算法。偶数个节点与寄数个节点健壮性效果是一样的，比如集群总共4个，半数以上是3个，只允许挂掉1个；集群总共3个，半数以上是2个，也是只允许挂掉1个。
- java客户端：zk自带的zkclient及Apache开源的Curator。
- 基于文件系统支持KV数据存储，


# 14. Zookeeper 选举流程

- leader 功能： 减少重复计算,提高性能,其他机器共享结果。
- 集群启动、启动后leader挂了、集群中follower数量不足半数、集群中增加Follower  这四种情景都会进行选举。
- [选举流程参考](https://www.cnblogs.com/longxok/p/8951867.html)


# 15. Eureka 特点

- **剔除服务机制**：默认周期为30秒进行检查某个实例心跳。默认90秒会注销该实例。
- **自我保护机制**：15分钟内超过85%的节点没有正常心跳，则认为出现了网络故障，进行自我保护模式(不再从注册表中移除因为长时间没有收到心跳而过期的服务).
- 是个servlet程序，直接集成到应用中，依赖于应用自身完成服务的注册与发现
- 遵循AP（可用性+分离容忍）原则，有较强的可用性，服务注册快，但牺牲了必定的一致性。无管理界面
- 服务发现感知很差，一般等心跳 才能发现，分钟级别的事情。
- Eureka 2.x目前已停止维护，但是Eureka 1.x 仍然是比较活跃的
- 客户端定时器：每隔30秒心跳、每隔30秒缓存刷新，多个定时器协同按需注册（本地服务地址时触发拉取、定时拉取、服务下线）

![](https://img2024.cnblogs.com/blog/1694759/202405/1694759-20240517140313241-665989300.png)

![](https://img2024.cnblogs.com/blog/1694759/202405/1694759-20240517140355515-1267487491.png)


# 16. Eureka 集群数据同步方式

- 主从复制 Master-Slave 模式: 主写从对
- 对等复制 Peer to Peer 模式: 各个服务都可以读写
- Eureka集群 采用Peer to Peer 模式。
- Eureka 通过 http header就是 HEADER_REPLICATION 解决循环复制问题。
- Eureka 通过 lastDirtyTimestamp 解决复制冲突。
- Eureka 通过心跳机制实现数据修复。

# 17. 如何实现服务发版流程不中断，优雅停机。

- 1.客户端短轮询方式,加长延迟停机时间，保证服务端先下线。
	- 客服端每30秒（eureka.client.registryFetchIntervalSeconds 配置项）从服务端拉去最新服务实例列表缓存。
	- 当服务端有实例下线时，服务端先向Eureka注册中心发送下线通知，服务端实例继续停止运行，待超过30秒后，在进行服务下线。
	- 可以保障客户端缓存里的服务地址不会失效，从而保障微服务的高可用。
	- 通过配置缩短心跳间隔(`eureka.instance.heartbeatIntervalInMilliseconds`)、缓存刷新间隔(`eureka.client.registryFetchIntervalSecond`)定时器提高服务发现的及时性。

- 2.客户端长轮询(long polling)方式,近实时从注册中心感知服务端下线。

- 3.应用端基于Ribbon调用对应的错误机制(链接超时、拒绝、404等错误),实现服务链表重试。

- 4.短轮询方式，结合Ribbon使用，Ribbon.ServerListRefreshInterval 设置客户端服务列表刷新间隔。

- 5.选用类似监听机制的注册中心，近实时感知服务端下线，如 Nacos

# 18. Consul 特点

- 遵循CP原则（一致性+分离容忍） 服务注册稍慢，因为其一致性致使了在Leader挂掉时从新选举期间consul不可用。
- go语言开发。独立外部应用 侵入性小。
- 也可以实现K/V数据存储， 虽然也支持配置中心，但更侧重于注册中心。。
- 也可以作为配置中心，不能自动更新。 要依赖 Consul-template 可实现动态配置，增加了部署和维护的复杂度 。
  - Consul原生不支持配置信息的历史版本管理，这对于需要追踪和回滚配置变更的场景不太友好
  - 通过@Value注入的属性，修改consul的配置后，属性不能立即生效，需要服务重启。而通过@ConfigurationProperties注入的属性，修改consul的配置后，属性会立即生效，所以建议如果需要动态生效的配置，最好使使用@ConfigurationProperties进行属性的注入。
- 使用raft算法实现选举，使用Gossip维护节点的元数据信息，进行信息交换。只有Leader可写，读可以是任意节点。
- Consul 默认情况下不自动注销实例，但可以通过配置健康检查（Health Checks）来实现自动注销。

```
可以设置 health-check-critical-timeout 参数来控制服务实例在标记为“critical”状态后多久自动注销。例如，以下配置表示如果服务在30秒内未响应健康检查，Consul 将自动注销服务：
service {
  name = "my-service"
  check {
    id      = "my-service-liveness"
    name    = "Service is alive"
    script  = "curl -s http://localhost:8080/health || exit 1"
    interval = "10s"
    timeout = "5s"
  }
  deregister_critical_service_after = "30s"  # 自动注销服务的超时时间
}

```


# 19. Nacos 特点

- 遵循CP原则（一致性+分离容忍） 和AP原则（可用性+分离容忍）。AP 是通过 Nacos 自研的 Distro 协议来保证的，CP 是通过 Nacos 的 Raft 协议来保证的。

- 支持注册中心(推荐AP)+ 配置中心(推荐CP)，默认采用AP模型，可以手动设置成CP模型。

	- 如果注册到Nacos的client节点注册时`ephemeral=true`(临时实例，基于心跳判断其状态)，是AP，采用distro协议实现；
      - 使用场景：动态微服务场景.
	- 如果注册到Nacos的client节点注册时`ephemeral=false`(永久实例，除非显示注销，牺牲一定的可用性保证数据一致性)，是CP的，采用raft协议实现。
      - 使用场景：固定服务或基础组件。
- Nacos可以根据业务和环境进行分组管理，提供服务标签数据，支持对服务在线管理。

- 有对应的后台管理页面，提供一些服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。

- Nacos 2.x 使用 GRPC 作为通信协议代替HTTP协议，支持长连接。

	- 心跳监测：

	  >在Nacos 2.x版本以后,使用Grpc协议代替了http协议，长连接会保持客户端和服务端发送的状态。临时实例走的是distro协议，持久实例则走的是Raft协议存储。主要有两种检测机制：
	  >1)、客户端主动上报机制(临时实例)：主动每5s向Nacos服务端发送心跳。
	  >2)、服务器端主动下探机制(持久实例)：Nacos服务端主动向每个Nacos客户端发起探活。
	  >NacosServer每3秒检测所有超过20S内没有发生过通讯的客户端，向客户端发起探测请求，如果客户端在指定时间内成功响应，则检测通过，否则执行下线。

	- 服务发现 (实时感知)：

	  > 客户端订阅相关服务,向 Nacos 服务器发送订阅请求。
	  > Nacos服务器接收到订阅请求后，会将订阅信息存储，并开始监听服务实例的变化。
	  > 若服务实例有更新（如新增、下线、健康状态改变等），Nacos 服务器会主动将这些变更推送给订阅了相关服务的客户端。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210806153311379-771263869.png)

Nacos 1.x 使用长轮询方式

![](https://img2024.cnblogs.com/blog/1694759/202405/1694759-20240524163557355-1045013368.png)


# 20. 各注册中心的比较

Etcd和Zookeeper的功能基本类似。都可以作为注册中心与配置中心。
- Zookeeper使用Java开发，会引入很多依赖。部署维护复杂，Paxos强一致性算法复杂难懂。发展更新缓慢
- Zookeeper的官方客户端只有Java和C语言两种。
- Etcd使用GO语言编写部署简单，使用HTTP+JSON接口使用简单，使用Raft算法。作为一个后起之秀。


|                | Zookeeper      | Eureka                                                       | Consul            | Nacos2.x                    | Etcd       |
|----------------|----------------| ------------------------------------------------------------ |-------------------|-----------------------------|------------|
| 数据一致性          | CP             | AP                                                           | CP                | AP(注册中心) / CP(配置中心)         | CP         |
| 一致性协议          | Zab            | --                                                           | Raft              | Distro / Raft                   | Raft       |
| 健康检查           | Keep Alive     | ClientBeat                                                   | TCP/HTTP/gRPC/cmd | TCP/HTTP/MySql/ClientBeat   | ClientBeat |
| Watch机制        | 有              | long polling                                                 | long polling      | 有(发布、订阅)                    | 有(发布、订阅)          |
| 自动注销实例         |                | √                                                            | × (默认无，需相关配置)     | √                           |            |
| 访问协议           | TCP            | HTTP                                                         | HTTP/DNS          | HTTP/DNS/gRpc               | HTTP       |
| 多数据中心          | ×              | √                                                            | √                 | √                           |            |
| 跨注册中心同步        | ×              | ×                                                            | √                 | 基于三方工具NacosSync 同步          |            |
| Spring Cloud集成 | √              | √                                                            | √                 | √                           |            |
| Dubbo集成        | √              | ×                                                            | ×                 | √                           |            |
| K8s集成          | ×              | ×                                                            | √                 | √                           | √          |
| 部署难度           | 4              | 1                                                            | 3                 | 2                           | 4          |
| 开发语言           | Java           | Java                                                         | Go                | Java                        | Go         |
| 异常服务下线时效性      | 心跳机制+临时节点，感知较强 | 取决于具体配置。默认 30s 更新服务实例信息，90s 才会去剔除失效的节点，在这种配置下可能 2 分钟才能获取到最新的配置 | 看具体配置             | 心跳检测机制,主动下探检测，20s左右下线       |            |




# 21. 设计注册中心要考虑哪些

- 服务自动注册与发现。服务端启动后自动注册，客户端拉取方式（轮询、长轮询，还是长连接双向通讯）。
	- 长连接: 一般使用WebSocket、gRPC(基于HTTP/2协议和Protocol Buffers设计)双向通信协议，需要更多的服务器和客户端支持，还可能存在防火墙等兼容问题。增加服务器资源销毁。
	- 长轮询: 基于HTTP/1.1，广泛支持，兼容性较好,另外实现简单，链接中断恢复简单。可以适当减少服务器资源占用。
	- 推模式：客户端与服务端建立好网络长连接,直接通过长连接通道推送到客户端.
		- 优点：数据变更后，服务端实时推送，客户端实现简单。
		- 缺点：客户端的数量比较多，服务端需要耗费大量的内存资源来保存每个资源，并且为了检测连接的有效性，还需要心跳机制来维持每个连接的状态。另外客户端会存在积压问题。
	- 拉模式：客户端主动向服务端发出请求，拉取相关数据
		- 优点：客户端发起请求，故不存在推模式中数据积压的问题
		- 缺点：数据不够及时，如果服务端配置长时间不更新的情况下，客户端的定时任务会做一些无效的Pull操作。

- 健康检查：心跳检测续约,自动下线。维持 注册中心与服务端心跳检测，自动续约或者下线机制。
- 支持集群，数据同步，CAP设计原则的选择，BASE 理论 。考虑CP 模式 还是AP模式？也可以仿照 Nacos 的CP+AP模式
- 访问协议 HTTP、DNS、TCP、gRPC？
- 当服务查询量很大的时候，怎么考虑性能。如：消费端缓存本地路由，提高路由效率和容错。在注册中心不可用时，消费者依然根据本地路由实现服务可靠调用
- 还要考虑消费者如何及时获取生产者变更问题
	- 1.发布-订阅模式，采用监听器或者回调机制。 参考 Zookeeper 的 watcher机制
	- 2.主动拉去策略，采用定时拉去最新服务列表更新本地缓存，例如 Eureka

- 考虑负载均衡、容错机制问题。提供些常见的负载均衡算法:轮询(加权)法、随机(加权)法、哈希算法、最小连接数法等，快速失败、失败重试。
- 如何支持多数据中心，多机房、考虑开发语言的技术栈,应用层面的使用
- 后台监控管理页面，可以对服务在线管理。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210806151736616-167930123.png)



- [大神分析](https://www.cnblogs.com/wtzbk/p/14071040.html) ：https://www.cnblogs.com/wtzbk/p/14071040.html

# 25. 配置中心选择与比较

- Nacos 与 Apollo 配置中心所有功能差不多，Nacos还支持注册中心更强大，Nacos良好的UI管理后台，有很多相关中文文档。
- Consul也支持配置中心、配置UI页面功能简陋、不支持配置回滚、不支持配置历史版本管理、还需要借助Consul Template实现自动更新，增加部署维护复杂度。
- Zookeeper也支持存储数据，功能简陋，无UI界面、权限粒度简单、不好用有些功能额外二开。

| 对比项目                 | Apollo                             | Nacos                      |
| ------------------------ | ---------------------------------- | -------------------------- |
| 开源时间                 | 2016.5                             | 2018.6                     |
| 配置实时推送             | 支持（HTTP长轮询1s内）             | 支持（HTTP长轮询1s内）     |
| 版本管理                 | 自动管理                           | 自动管理                   |
| 配置回滚                 | 支持                               | 支持                       |
| 权限管理                 | 支持（细粒度）                     | 粗粒度,无审核机制          |
| 灰度发布                 | 支持                               | 支持                       |
| 多集群多环境             | 支持                               | 支持                       |
| 监听查询                 | 支持                               | 支持                       |
| 多语言                   | Go,C++,Python,Java,.net,OpenAPI    | Python,Java,Nodejs,OpenAPI |
| 分布式高可用最小集群数量 | Config*2+Admin*3+Portal/*2+Mysql=8 | Nacos/*3+MySql=4           |
| 配置格式校验             | 支持                               | 支持                       |
| 通信协议                 | HTTP                               | HTTP                       |
| 单机读（tps）            | 9000                               | 15000                      |
| 单机写（tps）            | 1100                               | 1800                       |

### 结论

- 性能方面Nacos的读写性能最高，Apollo次之；
- 功能方面Apollo最为完善，但Nacos具有Apollo大部分配置管理功能。Nacos的一大优势是整合了注册中心、配置中心功能，部署和操作相比 Apollo都要直观简单，并减轻运维及部署工作。
- Apollo和Nacos生态支持都很广泛，在配置管理流程上做的都很好。Apollo在配置管理做的更加全面；Nacos则使用起来比较简洁，在对性能要求比较高的大规模场景更适合。
- 对于公司目前来说，修改配置的次数不是特别的频繁，对于配置权限的管理不是特别严格的，且对读写性能有一定要求的，可采用Nacos，反之使用Apollo。



# 26. 设计一个高性能的配置中心

- 考虑可用性与易用性：**对业务代码入侵小、统一配置管理UI页面**、支持集群高可用、多数据中心。
- 功能特性：**支持灰度发布、配置修改实时生效、用户角色权限、增加审核权限、操作审计、支持多tag环境、支持动态配置、告警通知、一键回滚、版本发布管理、本地缓存**
- 技术兼容性：兼容现有技术体系、支持springboot与springcloud生态
- 扩展性：提供开发平台API、提供各语言客户端。

