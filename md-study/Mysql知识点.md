
# 1.MySQL的在文件中是如何存储的？
- 数据是存在页中的，一页的大小是 16kb, 一个表由很多的页组成，这些页组成了 B+树。 每页之间数据结构是一个双向链表。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821143256977-1332888425.png)


# 2.Mysql如何更新一条数据的
- 当需要更新或者读取某条数据的时候，会把对应的页加载到内存中的 Buffer Pool 缓冲池中（默认为 128m 当然为了提高系统的并发度，你可以把这个值设置大一点）。当更新数据的时候，如果对应的页在 Buffer Pool 中，则直接更新 BP中的数据页即可，如果不在BP中，才会加载磁盘中对应的页到BP中，然后更新，此时BP中的页则跟磁盘中的页不一致，称为脏页。这些脏页是要被刷回磁盘中的。

	①. BP不够用了，要给新加载的页腾位置，所以会利用改进的 LRU 算法，将最近最久未使用的脏页刷回磁盘。
	②. 后台线程会在MySQL空闲的时候，将脏页刷回到磁盘中
	③. redolog写满时
	④. 数据库关闭时会将所有脏页刷回到磁盘中

# 3.如果脏页没有刷回，数据库宕机了怎么办？修改不就丢失了吗？

- 事务未提交，MySQL宕机，这种情况， Buffer Pool中的数据丢失，并且 redo log buffer中的日志也会丢失，不影响数据。
- 事务提交后，我们可以设置参数innodb_flush_log_at_trx_commit来决定redo log 的刷盘策略(设置为 1)：0 提交事务时，不会将redo log buffer中的数据写入os buffer，而是每秒写入os buffer并刷到磁盘；1 提交事务时，必须把redo log从内存刷入到磁盘文件中；2 提交事务时，将rodo log写入os buffer中，默认每隔1s将os buffer中的数据刷入磁盘。

# 4.Mysql 日志文件介绍

- 慢查询日志（slow query log）:记录慢查询的日志文件
- 查询日志（general log）:记录所有对数据库请求的信息
- 重做日志（redo log）: 记录更新的值，事务提交后生效。固定大写、循环写
- 回滚日志（undo log）: 记录原来的值，事务回滚时，用于恢复原来的值。
- 二进制日志（binlog）:用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步
- 错误日志（errorlog）:
- 中继日志（relay log）:

# 5.MVCC的实现
- MVCC 多版本并发控制,对于innodb存储引擎，每条行记录都包含两个隐藏列 trx_id、roll_pointer。
	-- trx_id 当前修改的事务id
	-- roll_pointer 每次对某条聚集索引记录进行改动时，都会把旧版本写入undo日志中。这个隐藏列就相当于一个指针，通过它找到该记录修改前的信息。
	-- 通过 ReadView+ UndoLog 实现的，UndoLog 保存了历史快照，ReadView 规则帮助判断当前版本的数据是否可见。
- 隔离级别 Read Committed(读已提交)，每次读取数据前都生成一个 ReadView
- 隔离级别 Repeatable Read（可重复读），只在第一次读取数据时生成一个 ReadView
- 通过乐观锁的机制解决**读写**之间的阻塞，降低死锁概率。
- **快照读**：不加锁的简单 Select 都属于快照读。
- **当前读**：就是读的是最新数据,而不是历史的数据，加锁的 SELECT，或者对数据进行增删改都会进行当前读。
- Read View 保存了当前事务开启时所有活跃的事务列表。可以理解为: Read View 保存了不应该让这个事务看到的其他事务 ID 列表。
	- trx_ids 系统当前正在活跃的事务ID集合。
	- low_limit_id ,活跃事务的最大的事务 ID。
	- up_limit_id 活跃的事务中最小的事务 ID。
	- creator_trx_id，创建这个 ReadView 的事务ID。
- Read View 规则：
	-- 如果 trx_id < 活跃的最小事务ID（up_limit_id）,也就是说这个行记录在这些活跃的事务创建前就已经提交了，那么这个行记录对当前事务是可见的。
	-- 如果trx_id > 活跃的最大事务ID（low_limit_id），这个说明行记录在这些活跃的事务之后才创建，说明这个行记录对当前事务是不可见的。
	-- 如果 up_limit_id < trx_id <low_limit_id,说明该记录需要在 trx_ids 集合中，可能还处于活跃状态，因此我们需要在 trx_ids 集合中遍历 ，如果trx_id 存在于 trx_ids 集合中，证明这个事务 trx_id 还处于活跃状态，不可见，否则 ，trx_id 不存在于 trx_ids 集合中，说明事务trx_id 已经提交了，这行记录是可见的。

- Read View 查询数据：

	- 获取事务自己的版本号，即 事务ID
	- 获取 Read View
	- 查询得到的数据，然后 Read View 中的事务版本号进行比较。
	- 如果不符合 ReadView 规则， 那么就需要 UndoLog 中历史快照；
	- 最后返回符合规则的数据


# 6.Innodb的事务与日志的实现方式
- 隔离级别：读未提交(RU)、读已提交(RC)、可重复读(RR)、Serializable串行化，mysql默认可重复读RR，mysql通过MVCC(多版本控制)和 Next-key lock(间隙锁)解决了幻读。
- MVCC是解决读写并行的幻读，而next-key lock 间隙锁 是解决写写并行的幻读。
- 间隙锁（Next-Key锁）:防止幻读,当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；
  对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。在Read Committed及RU隔离级别下，不会使用间隙锁。
- RR隔离级别**没有完全解决幻读问题**,同一个事务里面通过当前读(select for update之类、update、insert、delete)或者多个事务先更新然后快照读的形式来获取数据，就会产生幻读。
- RR快照读只会读取当前事务下数据的“历史态”，但当更新(dml、ddl）时，事务会去查看“当前态”某些数据行，验证数据的可执行性（如主键冲突、唯一性约束冲突等等）。一但有“当前态”的行数据被更新，这个行就会和当前”历史态“数据合并成新的”历史态“，此后该事务的快照读均是读取的新”历史态“快照。
- 当前读：对于会对数据修改的操作(update、insert、delete)都是采用当前读的模式。在执行这几个操作时会读取最新的版本号记录，写操作后把版本号改为了当前事务的版本号，所以即使是别的事务提交的数据也可以查询到。
  假设要update一条记录，但是在另一个事务中已经delete掉这条数据并且commit了，如果update就会产生冲突，所以在update的时候需要知道最新的数据。也正是因为这样所以才导致幻读。

- 幻读：指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

- 如何解决幻读？
    - 在快照读情况下,mysql通过mvcc来避免幻读。
    - 在当前读情况下通过X锁或Next-key来避免其他事物修改;
        - 使用串行化隔离级别
        - (update、delete)当where条件作为主键时,通过对主键索引加record locks来处理幻读。
        - (update、delete)当where条件为非主键索引时，通过next-key锁处理。next-key是record locks(索引加锁/行锁) 和 gap locks(间隙锁，每次锁住的不光是需要使用的数据，还会锁住这些数据附近的数据)的结合。
	
|事务隔离级别|脏读|不可重复读|幻读|
|:---:|:---:|:---:|:---:|
|读未提交RU|是|是|是|
|读未提交RC|否|是|是|
|读未提交RR|否|否|是|
|串行化|否|否|否|



# 7.聚集索引与非聚集索引

- 聚集索引： 数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引,一般指主键。
- 非聚集索引：除聚集索引外其他都是的，包括普通索引，唯一索引，全文索引。叶子节点并不包含行记录的全部数据.
- 使用非聚集索引查询，而查询列中包含了其他该索引没有覆盖的列，那么他还要进行第二次的查询，查询节点上对应的数据行的数据。

# 8.Mysql 索引失效   
- 1、查询时索引列不能为null值，因为索引是有序的，建索引时无法确定位置。
- 2、查询时 采用is Null,只能全表扫描。
- 3、数据库数据量较少时，mysql判断全表查询快时将不使用索引。
- 4、like查询以%开头，类型隐士转换、列参与函数运算、使用or查询，联合索引非最左

# 9.MySQL中Myisam与Innodb的区别
- InnoDB主键索引是聚簇索引，MyISAM索引是非聚簇索引。
- InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。
- MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。
- InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。
- InnoDB支持事物、支持行级锁(基于索引来完成行锁)，Myisam支持表级锁,不支持事物。
- InnoDB支持MVCC、外键，Myisam不支持,支持全文索引。

# 10.Innodb引擎四大特点

- **插入缓冲（insert buffer)**：先将数据插入或更新 写到Buffer Pool 缓冲池,然后再按一定频率一起写到硬盘，目的 减少IO带来的损耗。
- **二次写(double write)**：位于系统表空间的存储区域，用来缓存InnoDB的数据页从innodb buffer pool中flush之后并写入到数据文件之前
- **自适应哈希索引(ahi)**：某二级索引被频繁访问、减少B+树查询次数(2-4次)，系统自动建立哈希索引，加快查询速度
- **预读(read ahead)** : 如果一个w磁盘区(extent)中的被顺序读取的page超过或者等于该参数变量时，Innodb将会异步的将下一个extent读取到buffer pool中


# 11.mysql集群原理及方案

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821153146830-723209679.png)
![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821153204938-711203546.png)

# 12.创建索引的原则？
- 1.较为频繁查询条件的字段；
- 2.字段区分度、离散值较高的；
- 3.索引字段长度不能太长；
- 4.索引的个数不要太多，能复用组合索引尽量复用；
- 5.索引列尽量指定为NOT NULL；

# 13.索引下推ICP

- 就是将原本需要在**Server层**对数据进行过滤的条件下推到了**引擎层**去做，在引擎层过滤更多的数据，这样从引擎层发送到Server层的数据就会显著减少，从而优化性能。
- 在 InnoDB 中，ICP 只适用于二级索引.ICP 的目的就是为了减少回表导致的磁盘 I/O，而 InnoDB的聚簇索引的叶子节点存放的就是完整的数据记录，只要索引数据被读到内存了，那么索引项对应的完整数据记录也就读到内存了，那么通过索引项获取数据记录的过程就在内存中进行了，无需进行磁盘 I/O；也就说聚簇索引上应用 ICP，不会减少磁盘 I/O，也就没有使用的意义了。

- index key ：确定SQL查询在索引中的连续范围(起始范围+结束范围),扫描的索引数据范围。
- index filter ：根据 index key的索引范围 根据where条件进一步使用索引进行过滤。
- table filter：where条件中不能使用索引的， 只能 回表查询进行条件过滤。
- MySQL 5.6之前,不区分index filter和table filter， 回表读取完整记录，然后在server层过滤。
- Mysql 5.6及之后，index filter和table filter进行分离。index filter在引擎层进行过滤，减少回表查询的数据开销。

- 不支持覆盖索引、子查询条件的下推、存储过程条件、触发器条件的下推

# 14.count 字段区别
- 1.count(可空字段) 扫描全表，读到server层，判断字段可空，拿出该字段所有值，判断每一个值是否为空，不为空则累加
- 2.count(非空字段)与count(主键 id) 扫描全表，读到server层，判断字段不可空，按行累加。
- 3.count(1) 扫描全表，但不取值，server层收到的每一行都是1，判断不可能是null，按值累加。注意：count(1)执行速度比count(主键 id)快的原因：从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。
- 4.count(*) MySQL 执行count(*)在优化器做了专门优化。因为count(*)返回的行一定不是空。扫描全表，但是不取值，按行累加。


# 15.二叉树、B树、B+树
二叉树具有以下性质：左子树的键值小于根的键值，右子树
的键值大于根的键值。二叉树的查询效率就低了。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821154010052-779184076.png)

平衡二叉树（AVL树）在符合二叉查找树的条件下，还满足任何节点的两个子树的高度最大差为1。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821154037213-1789401676.png)


平衡多路查找树（B-Tree），系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来。InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位，默认16K。InnoDB每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小16KB。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821154057316-1843035106.png)


缺点：每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。

B+Tree相对于B-Tree有几点不同：
非叶子节点只存储键值信息。
所有叶子节点之间都有一个链指针。
数据记录都存放在叶子节点中

在数据库中，B+Tree的高度一般都在2~4层。mysql的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。

![](https://img2020.cnblogs.com/blog/1694759/202108/1694759-20210821154117840-1597722525.png)


数据库索引采用B+树的主要原因是：B树在提高了IO性能的同时并没有解决元素遍历的我效率低下的问题，正是为了解决这个问题，B+树应用而生。
B+树只需要去遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低。



#  16. B+ 树能存放多少条数据？

- 总记录数为 = **根节点指针数\*单个叶子节点记录行数 **。
- 比如聚集索引，主键ID为bigInt 长度为8 字节，innoDB 源码设置为6字节，一条记录就是14字节。一页能放 16*1024÷14 = 1170
- 假如每条数据大小为1K, 一页能存放16k/1k = 16.
- 高度为3的能放 1170\*1170\* 16 = 219002400，大概2200万左右。
- 如果是 非聚集索引，叶子结点存放的是聚集索引(主键Id)，则能存放的数量更大。


# 17.LSM树（Log Structured Merge Trees）原理

- 将对数据的修改增量保存在内存中，达到指定大小限制之后批量把数据flush到磁盘中，磁盘中树定期可以做merge操作，合并成一棵大树，以优化读性能。不过读取的时候稍微麻烦一些，读取时看这些数据在内存中，如果未能命中内存，则需要访问较多的磁盘文件。
- LSM牺牲了部分读性能，提高写性能。适用于写多读少的场景。主要劣势：读写放大（磁盘上实际读写的数据量 / 用户需要的数据量）。
- LSM是当前被用在许多产品的文件结构策略：HBase,TIDB,mangoDb 等。
- LSM树的核心特点是利用顺序写来提高写性能，但因为分层(此处分层是指的分为内存和文件两部分)的设计会稍微降低读性能，但是通过牺牲小部分读性能换来高性能写，使得LSM树成为非常流行的存储结构。

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211119204033094-907558566.png)

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211119205246316-371070524.png)

![](https://img2020.cnblogs.com/blog/1694759/202111/1694759-20211119205331509-1561924404.png)



# 18.如何解决幻读？
- 在快照读情况下,mysql通过MVCC来避免幻读。
- 在当前读情况下通过X锁或Next-key来避免其他事物修改;
	- 使用串行化隔离级别
	- (update、delete)当where条件作为主键时,通过对主键索引加record locks来处理幻读。
	- (update、delete)当where条件为非主键索引时，通过next-key锁处理。next-key是record locks(索引加锁/行锁) 和 gap locks(间隙锁，每次锁住的不光是需要使用的数据，还会锁住这些数据附近的数据)的结合。









